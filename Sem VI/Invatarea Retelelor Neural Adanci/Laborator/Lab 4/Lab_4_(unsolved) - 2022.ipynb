{"cells":[{"cell_type":"markdown","metadata":{"id":"NjZTLbz4Vx4c"},"source":["<font size=25>Laboratory 4 summary</font>\n","\n","In this lab you will gain debugging experience by solving the most typical deep learning bugs. \n","\n","There are 8 exercises, each one with a corresponding cell. Run the cell, inspect the error and fix the code. \n","\n","The bugs can be fixed by editing one or two lines of code. Try not to inspect other cells while solving the current one."]},{"cell_type":"markdown","metadata":{"id":"HUoo-4E12opk"},"source":["# **Exercises**\n","\n","Run the cell below to import the packages, which are required for all the exercises below."]},{"cell_type":"code","source":["from __future__ import print_function, division\n","import os\n","import torch\n","import random\n","from typing import Iterator, List, Callable, Tuple\n","from functools import partial\n","import warnings\n","from math import *\n","import zipfile\n","from tqdm import tqdm\n","from PIL import Image\n","\n","# Sklearn\n","from sklearn.datasets import load_digits\n","# Numpy\n","import numpy as np\n","# Pandas\n","import pandas as pd\n","\n","# PyTorch packages\n","from torch.utils.data import Dataset, TensorDataset, DataLoader\n","from torch.utils.data import RandomSampler, Sampler\n","from torchvision import transforms, utils, datasets\n","from torchvision.transforms import ToTensor, ToPILImage\n","import torch.nn as nn\n","\n","# matplotlib\n","from matplotlib import rc, cm\n","rc('animation', html='jshtml')\n","import matplotlib.pyplot as plt\n","from mpl_toolkits import mplot3d\n","import matplotlib.animation as animation\n","%matplotlib notebook\n","#warnings.filterwarnings(\"ignore\")\n","plt.ion()   # interactive mode"],"metadata":{"id":"8vvjpuXSFmFD","executionInfo":{"status":"ok","timestamp":1646672077017,"user_tz":-120,"elapsed":3643,"user":{"displayName":"Florin Brad","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06472899283008656976"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BsINHO_NwDv5"},"source":["## Exercise 1: Getting started\n"]},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 2)\n","        self.activation_fn = activation_fn\n","\n","    def forward(self, x):\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","model = MLP(input_size=100, hidden_size=256, activation_fn=nn.ReLU())\n","\n","x = torch.rand(32, 200)\n","\n","y = model(x)\n","assert y.shape[0] == 32 and y.shape[1] == 2, \"Wrong output shape\""],"metadata":{"id":"k1BvB8TwEPzx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 2: Getting in shape"],"metadata":{"id":"uHW1o4myBc4m"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 2)\n","        self.activation_fn = activation_fn\n","\n","    def forward(self, x):\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","model = MLP(input_size=784, hidden_size=256, activation_fn=nn.ReLU())\n","mnist_trainset = datasets.MNIST(\n","    root='./data', \n","    train=True, \n","    download=True, \n","    transform=transforms.ToTensor())\n","\n","x, l = mnist_trainset[10]\n","y = model(x)\n","\n","assert y.shape[0] == 1 and y.shape[1] == 2, \"Wrong output shape\""],"metadata":{"id":"3GCJ9iNHNFIe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 3: It's the little things\n"],"metadata":{"id":"3beV3q_MBf4u"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 2)\n","        self.activation_fn = activation_fn\n","\n","    def forward(self, x):\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(x)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","model = MLP(input_size=784, hidden_size=256, activation_fn=nn.ReLU)\n","\n","x = torch.rand(32, 784)\n","y = model(x)\n","assert y.shape[0] == 32 and y.shape[1] == 2, \"Wrong output shape\""],"metadata":{"id":"ijj1R6SXP-fc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 4: No one left behind"],"metadata":{"id":"QAJtebM_Bjck"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 batch_size: int,\n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 10)\n","        self.batch_size = batch_size\n","        self.activation_fn = activation_fn\n","\n","    def forward(self, x):\n","        x = x.view(self.batch_size, -1)\n","\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","# instantiate model\n","BATCH_SIZE=32\n","model = MLP(\n","    input_size=784, \n","    hidden_size=256, \n","    activation_fn=nn.ReLU(), \n","    batch_size=BATCH_SIZE\n",")\n","\n","# instantiate MNIST dataset\n","val_dataset = datasets.MNIST(\n","    root='./data', \n","    train=False, \n","    download=True, \n","    transform=transforms.ToTensor())\n","print(\"validation dataset size = \", len(val_dataset))\n","\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True\n",")\n","\n","loss_crt = nn.CrossEntropyLoss()\n","epoch_loss = 0.0\n","for batch_images, batch_labels in val_dataloader:\n","    # batch_size x 2\n","    out = model(batch_images)\n","    loss = loss_crt(out, batch_labels)\n","    epoch_loss += loss.item()\n","\n","epoch_loss /= len(val_dataloader)\n","print(\"Validation loss = \", epoch_loss)"],"metadata":{"id":"cbXjL4t37_JZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 5: Left to their own devices\n"],"metadata":{"id":"BvVmqB1tHSF4"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 batch_size: int,\n","                 device: torch.device,\n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 10)\n","        self.activation_fn = activation_fn\n","        self.device = device\n","        self.batch_size = batch_size\n","\n","    def forward(self, x):\n","        # move Tensor to GPU (if available)\n","        x.to(self.device)\n","\n","        # reshape tensor\n","        # batch_size x 784\n","        x = x.view(self.batch_size, -1)\n","\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","BATCH_SIZE=32\n","device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n","print(\"device = \", device)\n","\n","# instantiate model\n","model = MLP(\n","    input_size=784, hidden_size=256, activation_fn=nn.ReLU(), batch_size=32,\n","    device=device\n",")\n","\n","# move model to GPU (Module.to() is an in-place operation)\n","model.to(device)\n","\n","# instantiate MNIST dataset\n","train_dataset = datasets.MNIST(\n","    root='./data', \n","    train=True, \n","    download=True, \n","    transform=transforms.ToTensor())\n","print(\"train dataset size = \", len(train_dataset))\n","\n","# instantiate dataloader\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n",")\n","\n","loss_crt = nn.CrossEntropyLoss()\n","epoch_loss = 0.0\n","for batch_images, batch_labels in train_dataloader:\n","    # move labels to GPU (if available)\n","    batch_labels.to(device)\n","    \n","    # batch_size x 2\n","    # feedforward\n","    out = model(batch_images)\n","    \n","    # compute loss \n","    loss = loss_crt(out, batch_labels)\n","    epoch_loss += loss.item()"],"metadata":{"id":"bymjthPlMBY3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 6: Not exactly my type"],"metadata":{"id":"6xEcE46Y9q74"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 device: torch.device,\n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 10)\n","        self.activation_fn = activation_fn\n","        self.device = device\n","\n","    def forward(self, x):\n","        # batch_size x 64\n","        x = x.to(self.device)\n","\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","BATCH_SIZE=32\n","device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n","print(\"device = \", device)\n","\n","# instantiate model\n","model = MLP(\n","    input_size=64, hidden_size=256, activation_fn=nn.ReLU(), device=device\n",")\n","\n","# move model to GPU (Module.to() is an in-place operation)\n","model.to(device)\n","\n","# load the 1797 images from the Digits dataset:\n","# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html\n","# Images are grayscale digits from 0 to 9, stored as arrays of size 64 (8x8). \n","# Both images and labels are stored are NumPy arrays, so we need to convert \n","# them to Tensors.\n","x = load_digits()\n","\n","# 1797 x 64, 1797\n","images, labels = torch.tensor(x.data), torch.tensor(x.target)\n","\n","# we create a TensorDataset, which is a type of Dataset that wraps Tensors.\n","# https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset\n","# Examples are indexed over the first dimension, so the first dimension of \n","# the Tensors must be the same (1797 in our case)\n","train_dataset = TensorDataset(images, labels)\n","\n","# instantiate dataloader\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n",")\n","\n","loss_crt = nn.CrossEntropyLoss()\n","epoch_loss = 0.0\n","for batch_images, batch_labels in train_dataloader:\n","    batch_labels=batch_labels.to(device)\n","    \n","    # batch_size x 2\n","    # feedforward\n","    out = model(batch_images)\n","    \n","    # compute loss \n","    loss = loss_crt(out, batch_labels)\n","    epoch_loss += loss.item()"],"metadata":{"id":"WDdeFJtSW0ea"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 7: Out of bounds\n","The [Wheat Seeds](https://archive.ics.uci.edu/ml/datasets/seeds) dataset ([Kaggle link](https://www.kaggle.com/jmcaro/wheat-seedsuci)) is a classification task with 3 classes, which contains 209 examples. Each example contains 7 geometrical properties of wheat seeds belonging to 3 varieties of wheat. \n","\n","**Hint 1:** When training on GPUs, CUDA errors may be less helpful. Usually, errors such as \"`RuntimeError: CUDA error: device-side assert triggered`\" indicate a problem with an index, which may be too large. To get a more accurate error message, move the model and dataset to CPU, check the error again and try to fix it.\n","\n","**Hint 2:** After fixing the code responsible for a CUDA error, you may still encounter the error when running on GPU. Try restarting the Colab Notebook (`Runtime` -> `Restart runtime`) and run the cells again.\n"],"metadata":{"id":"1WRHhMa4nX8R"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 device: torch.device,\n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.hidden_layer = nn.Linear(input_size, hidden_size)\n","        self.output_layer = nn.Linear(hidden_size, 3)\n","        self.activation_fn = activation_fn\n","        self.device = device\n","\n","    def forward(self, x):\n","        # batch_size x 7\n","        x = x.to(self.device)\n","\n","        h = self.hidden_layer(x)\n","        h = self.activation_fn(h)\n","        out = self.output_layer(h)\n","\n","        return out\n","\n","BATCH_SIZE=32\n","\n","# if you encounter a vague CUDA error message, move the operations to CPU then\n","# run the code again. The error message is usually more helpful.\n","device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n","print(\"device = \", device)\n","\n","# instantiate model\n","model = MLP(\n","    input_size=7, hidden_size=128, activation_fn=nn.ReLU(), device=device\n",")\n","\n","# move model to GPU (Module.to() is an in-place operation)\n","model.to(device)\n","\n","# download Wheat Seeds dataset\n","!wget --no-check-certificate \\\n","https://raw.githubusercontent.com/jbrownlee/Datasets/master/wheat-seeds.csv \\\n","-O /tmp/wheat.csv\n","\n","# read Wheat Seeds dataset from csv\n","# Dataset has 209 examples. Each example has 7 attributes (features).\n","# It's a classification task with 3 classes (1, 2 and 3)\n","data = pd.read_csv(\"/tmp/wheat.csv\")\n","\n","# put examples in a Tensor\n","x = torch.tensor(data.values, dtype=torch.float32)\n","\n","# separate data and labels\n","data, labels = x[:,:-1], x[:,-1].long()\n","\n","# we create a TensorDataset, which is a type of Dataset that wraps Tensors.\n","# https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset\n","# Examples are indexed over the first dimension, so the first dimension of \n","# the Tensors must be the same (209 in our case)\n","validation_dataset = TensorDataset(data, labels)\n","\n","# instantiate dataloader\n","validation_dataloader = DataLoader(\n","    validation_dataset,\n","    batch_size=BATCH_SIZE\n",")\n","\n","loss_crt = nn.CrossEntropyLoss()\n","epoch_loss = 0.0\n","for batch_images, batch_labels in validation_dataloader:\n","    batch_labels=batch_labels.to(device)\n","    \n","    # feedforward\n","    # batch_size x 3\n","    out = model(batch_images)\n","    \n","    # compute loss \n","    loss = loss_crt(out, batch_labels)\n","    epoch_loss += loss.item()\n","\n","epoch_loss /= len(validation_dataloader)\n","print(\"Validation loss = \", epoch_loss)"],"metadata":{"id":"xZP5yCbeo5Pd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 8: I have no memory of that\n","\n","**Hint 1:** The error will appear after ~2 epochs\n","\n","**Hint 2:** You do NOT need to modify the model's size to fix the memory bug\n","\n","**Hint 3:** After getting the error message, you have to restart the machine:\n","  - restart Colab: `Runtime` -> `Restart runtime`\n","  - run the cell that imports packages \n","  - run the cell below"],"metadata":{"id":"4v5D0k3MHsW0"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size_1: int, \n","                 hidden_size_2: int, \n","                 hidden_size_3: int, \n","                 hidden_size_4: int, \n","                 device: torch.device,\n","                 activation_fn: Callable):\n","        super().__init__()\n","        self.input_size = input_size\n","        self.hidden_size_1 = hidden_size_1\n","        self.hidden_size_2 = hidden_size_2\n","        self.hidden_layer_1 = nn.Linear(input_size, hidden_size_1)\n","        self.hidden_layer_2 = nn.Linear(hidden_size_1, hidden_size_2)\n","        self.hidden_layer_3 = nn.Linear(hidden_size_2, hidden_size_3)\n","        self.hidden_layer_4 = nn.Linear(hidden_size_3, hidden_size_4)\n","        self.output_layer = nn.Linear(hidden_size_4, 10)\n","        self.activation_fn = activation_fn\n","        self.device = device\n","\n","    def forward(self, x):\n","        # move input data to GPU (if available)\n","        x = x.to(self.device)\n","\n","        # reshape tensor\n","        # batch_size x 784\n","        x = x.view(-1, self.input_size)\n","\n","        h1 = self.activation_fn(self.hidden_layer_1(x))\n","        h2 = self.activation_fn(self.hidden_layer_2(h1))\n","        h3 = self.activation_fn(self.hidden_layer_3(h2))\n","        h4 = self.activation_fn(self.hidden_layer_4(h3))\n","        out = self.output_layer(h4)\n","\n","        return out\n","\n","BATCH_SIZE=32\n","device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu')\n","print(\"device = \", device)\n","\n","# instantiate model\n","model = MLP(\n","    input_size=784, \n","    hidden_size_1=4096,\n","    hidden_size_2=4096,\n","    hidden_size_3=4096,\n","    hidden_size_4=4096,\n","    activation_fn=nn.ReLU(), \n","    device=device\n",")\n","\n","# move model to GPU (Module.to() is an in-place operation)\n","model.to(device)\n","\n","# instantiate MNIST dataset\n","train_dataset = datasets.MNIST(\n","    root='./data', \n","    train=True, \n","    download=True, \n","    transform=transforms.ToTensor())\n","print(\"train dataset size = \", len(train_dataset))\n","\n","# instantiate dataloader\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n",")\n","\n","loss_crt = nn.CrossEntropyLoss()\n","epoch_loss = 0.0\n","num_batches = len(train_dataloader)\n","for epoch in range(20):\n","    for idx, (batch_images, batch_labels) in enumerate(train_dataloader):\n","        if idx % 50 == 0:\n","            print(\"epoch %d, batch %d/%d\" % (epoch, idx, num_batches))\n","\n","        # move labels to GPU (if available)\n","        batch_labels=batch_labels.to(device)\n","        \n","        # batch_size x 2\n","        # feedforward\n","        out = model(batch_images)\n","        \n","        # compute loss \n","        loss = loss_crt(out, batch_labels)\n","\n","        epoch_loss += loss\n","\n","    epoch_loss /= num_batches\n","    print(\"epoch loss = \", epoch_loss.item())"],"metadata":{"id":"mAahmqrJae1h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"o_XMAAIA9r0u"}}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Lab_4_(unsolved) - 2022.ipynb","toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyOuHVJrgRumoM9MEgXwxIBv"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}