{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab_5_(unsolved)-2022.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyM7PDtCPQQ+W5jliqa5s5KA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["<font size=25, color='#ED1F24'>Laboratory 5\n","\n","<font size=25, color='#ED1F24'>Convolutional Neural Networks (CNNs)"],"metadata":{"id":"XeM0PutfeJUK"}},{"cell_type":"markdown","source":["**Summary:**\n","\n","\n","*   learn about convolutions \n","*   learn to build Convolution Neural Networks (CNNs)\n","*   train CNNs\n","*   use pretrained models\n","\n","\n","\n"],"metadata":{"id":"lZBHTLKDqdz5"}},{"cell_type":"markdown","source":["**Motivation**\n","\n","Fully Connected Layer \n","\n","<div>\n","<img src=https://drive.google.com/uc?id=1xaImbI9cZfaMPjc1UfG8OXymqYAl3aCN width=\"600\"/>\n","</div>\n","\n","---\n","Images\n","<div>\n","<img src=https://drive.google.com/uc?id=1MZ64Ay5PIozenvyIzN1TveNEj-ATqnLd width=\"600\"/>\n","<div>\n","\n","---\n","\n","In order to use a fully connected layer for processing an image, we need to flatten the image ($⇒$ get a vector). The input size of our layer will be equal to the number of pixels contained in the image, multiplied by the number of channels. So, if we are working with RGB images, at a resolution of ($width$, $height$), the input size would be $3 * width * height$.\n","\n","After image flattening, the input would look something like this:\n","<div>\n","<img src=https://drive.google.com/uc?id=1I5nv1Ly6EUAAZbdntYxId2o7CKD78SCa width=\"1000\"/>\n","<div>\n","\n","This is an undesirable approach and during this laboratory, we will explore an alternative approach.\n","\n","---\n","\n"],"metadata":{"id":"CwBD-lNQVfSs"}},{"cell_type":"markdown","source":["**Question 1** \n","If we stack multiple fully connected layers of the form $y=Wx$, can we model complex functions?\n","\n","**Question 2** Do we have any limitation regarding the functions that we could model using a fully connected network (composed of multiple fully connected layers, with non-linear activations) ? "],"metadata":{"id":"BYqYfbjHu2FC"}},{"cell_type":"markdown","source":["Main drawbacks of Fully Connected Networks:\n","\n","*   many parameters $⇒$ require a lot of training examples\n","*   each pixel is treated differently $⇒$ it has to learn redundant features\n","*   small image translations would induce different outputs\n","\n","Insights for processing images (useful biases):\n","\n","*   close neighbors of one pixel are more relevant than distant ones\n","*   an observed pattern should have the same meaning regardless its position in the image\n","\n","\n","\n","\n","\n"],"metadata":{"id":"e3_uHVdHwuzG"}},{"cell_type":"markdown","source":["Convolutional Neural Networks (CNNs) exploit the above mentioned biases and tackle the main drawbacks of Fully Connected Networks. \n","\n","<div>\n","<img src=https://drive.google.com/uc?id=1PtO6mNFtm71ftrn2ea0huyvEk9PpmXSd width=\"750\"/>\n","<div>\n","\n","*Image Source: [FloydHub](https://blog.floydhub.com/building-your-first-convnet/)*\n"],"metadata":{"id":"dZWAOi3rdgFU"}},{"cell_type":"markdown","source":["CNNs can be employed for multiple modalities:\n","\n","*   computer vision \n","*   speech recognition and speech synthesis \n","*   natural language processing \n","*   protein/DNA binding problem \n","*   any problem with a spatial (or sequential) structure\n","\n"],"metadata":{"id":"qwhducGtaJ6d"}},{"cell_type":"markdown","source":["# <font color='ED1F24'>Part I: Convolutions \n","\n"],"metadata":{"id":"EJCxoDb_e7oa"}},{"cell_type":"markdown","source":["\n","Convolution is the mathematical way of combining two signals to form a third signal. \n","\n","In the current laboratory, we will discuss about convolutions in the context of image processing. \n","\n","Convolution in image processing = the process of transforming an image by applying a kernel over each pixel and its local neighbors across the entire image.\n","\n","The most common type of convolutions are:\n","\n","*   1D convolutions (e.g. temporal data)\n","*   2D convolutions (e.g. images with height and width)\n","*   3D convolutions (e.g. videos with height, width and time)\n","\n","Informal: How to think about xD convolutions? We are working over data with x main dimensions along which we expect a neighbourhood to have a specific meaning. Each xD point can additionally have multiple features (channels).\n","\n","E.g.:\n","  - 1D convolutions: assume we follow chocolate sales, collecting data at regular time intervals, regarding price, marketing spend and if is weekend or not; we expect a certain temporal consistency between neighbor data samples \n","  - 2D convolutions: images are 3D tensors, where each pixel has 3 color channels (RGB features); we expect a certain spatial consistency between neighbor pixels \n"],"metadata":{"id":"j_AJeYsjiO8-"}},{"cell_type":"code","source":["import torch \n","import torch.nn as nn\n","from torchvision import datasets, transforms\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from tqdm import tqdm\n","from typing import Iterator, List, Callable, Tuple\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","torch.manual_seed(115)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"SYdDlSWUtu1_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2D convolutions over single-channel data \n","\n","\n"],"metadata":{"id":"9tzLIi90SgnH"}},{"cell_type":"markdown","source":["We will start working over single-channel images (grayscale images):\n","<div>\n","<img src=https://drive.google.com/uc?id=19lH10oZetZ-VFOAJUso4FR4l9Q7GPzPJ width=\"600\"/>\n","<div>"],"metadata":{"id":"ehgi0-Ra049q"}},{"cell_type":"markdown","source":["**Convolution** \n","Over a $4\\times 4$ image, with a $3\\times 3$ kernel\n","\n","<div>\n","<img src=https://drive.google.com/uc?id=1uiSMfwIsmR72m6j7PGsknzemp9S1GGFc width=\"200\"/>\n","<div>\n","\n","*Image Source: [GitHub](https://github.com/vdumoulin/conv_arithmetic)*\n","\n","<div>\n","<img src=https://drive.google.com/uc?id=1w2V0Ew6V6_WExaQop02G_YAQ0g4ZiifW width=\"500\"/>\n","<div>\n"],"metadata":{"id":"4-RxbqbvwM9h"}},{"cell_type":"code","source":["# initialize a single channel image of size 4 x 4\n","image = torch.arange(0,16).reshape((4,4))\n","# initialize a 3 x 3 kernel\n","kernel = torch.tensor([[1,0,-1],[0,1,0],[-1, 0, 1]]) \n","\n","print('The image:')\n","print(image)\n","print('The kernel:')\n","print(kernel)"],"metadata":{"id":"vz_R6nOKDKZj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO - complete the function implementing the above convolution\n","def convolution(image, kernel):\n","  in_height, in_width = image.shape\n","  kernel_height, kernel_width = kernel.shape\n","  out_height = ...\n","  out_width = ...\n","  result = torch.zeros((out_height, out_width))\n","  for i in range(out_height):\n","    for j in range(out_width):\n","      image_crop = ...\n","      result[i,j] = ...\n","  return result\n","\n","res = convolution(image, kernel)\n","print('Result:')\n","print(res) "],"metadata":{"id":"EM80NffbCTuK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see that the implemented convolution reduces the image size. If we wish to maintain the original size, we can add a border around the initial image. \n","\n","**Padding** - prevents shrinking by adding an image border before convolution\n","<div>\n","<img src=https://drive.google.com/uc?id=19OqGj6PpoBT50HQ8q0mKhhESaz8QGk-- width=\"200\"/>\n","<div>\n","\n","*Image Source: [GitHub](https://github.com/vdumoulin/conv_arithmetic)*\n","\n","\n","\n"],"metadata":{"id":"-cp1_dNQKeI-"}},{"cell_type":"code","source":["# TODO - complete the function implementing the above convolution with padding \n","# the image will be zero padded\n","def convolution_with_padding(image, kernel, padding):\n","  in_height, in_width = image.shape\n","  kernel_height, kernel_width = kernel.shape\n","  out_height = ... \n","  out_width = ...\n","  result = torch.zeros((out_height, out_width))\n","  # add border \n","  image = ...\n","  for i in range(out_height):\n","    for j in range(out_width):\n","      image_crop = ...\n","      result[i,j] = ...\n","  return result\n","\n","padding = 1\n","res = convolution_with_padding(image, kernel, padding)\n","print('Result:')\n","print(res) "],"metadata":{"id":"qEM2DQrgME0_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Edge detection\n"],"metadata":{"id":"rveT2k282Ysz"}},{"cell_type":"markdown","source":["\n","Detect sharp changes in image brightness, which are likely to correspond to discontinuities in depth, discontinuities in surface orientation, illumination variations or texture changes. ([more about edges](https://en.wikipedia.org/wiki/Edge_detection)). \n","\n","We will implement a simple edge detector, using the [Sobel operator](https://en.wikipedia.org/wiki/Sobel_operator).\n","\n","The operator uses two $3\\times3$ kernels which are convolved with the original image. The result approximates the image derivatives $⇒$ brightness changes along the horizontal and vertical axis. Further, using this maps we can obtain the magnitude of the gradient and estimate the edge intensity in each pixel."],"metadata":{"id":"wXOI8kDG7G-v"}},{"cell_type":"markdown","source":["**Estimate vertical edges**\n","\n","In order to estimate the vertical edges, we need to look for brightness changes along the horizontal dimension. To achieve this, we can use the following Sobel filter:\n","\n","\\begin{bmatrix}\n","  -1 & 0 & 1\\\\ \n","  -2 & 0 & 2 \\\\\n","  -1 & 0 & 1\n","\\end{bmatrix}\n","\n","The filter is convolved over the image, resulting in a map that contains in each pixel the approximation of the image derivative along the horizontal dimension. \n","\n"],"metadata":{"id":"q8tCSuFWp5cN"}},{"cell_type":"code","source":["# TODO - define a function that computes the image derivatives along the horizontal axis\n","def sobel_vertical_edges(image):\n","  # initialize the kernel/filter\n","  kernel_x = torch.tensor(...)\n","  # apply the kernel over the image \n","  edges_x = ...\n","  return edges_x\n","\n","# TODO - replace with path to the chessboard.png image \n","synthetic_image_path = '/content/gdrive/MyDrive/course_fmi_2022/Lab 5/chessboard.png'\n","transf = transforms.ToTensor()\n","image = transf(Image.open(synthetic_image_path).convert(mode='L'))[0]\n","edges_x = sobel_vertical_edges(image)\n","\n","print('Original Image')\n","plt.imshow(image, cmap='gray')\n","plt.show()\n","print('Vertical Edges')\n","plt.imshow(abs(edges_x), cmap='gray')"],"metadata":{"id":"CzJk4Gto0DTh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Estimate horizontal edges**\n","In order to estimate the horizontal edges, we need to define a second filter to be convolved over the image such that the convolution result approximates the image derivatives along the vertical dimension. \n"],"metadata":{"id":"O0YZwxo3vKWS"}},{"cell_type":"code","source":["# TODO - define a function that computes the image derivatives along the vertical axis\n","def sobel_horizontal_edges(image):\n","  ...\n","  return edges_y\n","\n","edges_y = sobel_horizontal_edges(image)\n","\n","print('Original Image')\n","plt.imshow(image, cmap='gray')\n","plt.show()\n","print('Horizontal Edges')\n","plt.imshow(abs(edges_y), cmap='gray')"],"metadata":{"id":"JUoHdx7X7DuO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Edges Map**\n","\n","Using the horizontal and vertical edges, we can compute the final edges map. \n","Let $G_x$ be the vertical edges and  $G_y$ the horizontal ones. \n","\n","The gradient magnitude can be computed as follows:\n","$G = \\sqrt{G_x^2+G_y^2}$"],"metadata":{"id":"TeeLEtjdC-wb"}},{"cell_type":"code","source":["# TODO - define a function that given an input image, estimates the edges map\n","# the function will return the edges map along with the maps corresponding to vertical and horizontal edges\n","def sobel_edges(image):\n","  ...\n","  return edges, edges_x, edges_y\n","\n","edges, _, _ = sobel_edges(image)\n","\n","print('Original Image')\n","plt.imshow(image, cmap='gray')\n","plt.show()\n","print('Edges')\n","plt.imshow(edges, cmap='gray')\n","plt.show()"],"metadata":{"id":"uvVLH38_EHMI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# now, let's apply the edge detector over a real image \n","# TODO - replace with path to the city.png image \n","real_image_path = '/content/gdrive/MyDrive/course_fmi_2022/lab5_images/city.png'\n","transf = transforms.ToTensor()\n","image = transf(Image.open(real_image_path).convert(mode='L'))[0]\n","\n","edges, edges_x, edges_y = sobel_edges(image)\n","print('Original Image')\n","plt.imshow(image, cmap='gray')\n","plt.show()\n","print('Vertical Edges')\n","plt.imshow(edges_x, cmap='gray')\n","plt.show()\n","print('Horizontal Edges')\n","plt.imshow(edges_y, cmap='gray')\n","plt.show()\n","print('Edges')\n","plt.imshow(edges, cmap='gray')\n","plt.show()"],"metadata":{"id":"GErNXO4NE2yI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2D convolutions over multi-channel data\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"8_xfiF9aSrfH"}},{"cell_type":"markdown","source":["We will illustrate convolutions over three-channel images (RGB images):\n","<div>\n","<img src=https://drive.google.com/uc?id=1MZ64Ay5PIozenvyIzN1TveNEj-ATqnLd width=\"600\"/>\n","<div>\n","\n","The operations can be extrapolated to higher numbers of channels.\n","\n","<div>\n","<img src=https://drive.google.com/uc?id=1Vu2CCnsmH7EGR5_nTDfh7ygOXnIP83CG width=\"600\"/>\n","<div>\n","\n","*Image Source: [link](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)*"],"metadata":{"id":"IssOSIn61abC"}},{"cell_type":"markdown","source":["Note: we will make use of the already implemented functions, and in the next section we will learn how to efficiently use the PyTorch implementation for implementing convolutions. "],"metadata":{"id":"KcLLfwq22eqY"}},{"cell_type":"markdown","source":["### Compute Grayscale Images\n","\n"],"metadata":{"id":"6vJ0__JY2DZm"}},{"cell_type":"markdown","source":["Let $R$, $G$ and $B$ be the red, green and blue channels of an image. \n","\n","We will consider the weighted method for computing the grayscale representation of the image, as follows:\n","\n","$grayscale = 0.299 R + 0.587 G + 0.114 B$"],"metadata":{"id":"YCs1E0sOAtvM"}},{"cell_type":"code","source":["# TODO - we will implement the color conversion using convolution operations \n","# Note: this is a highly ineficient approach and is used solely for exemplification\n","def rgb_2_grayscale(image):\n","  # define the kernel\n","  kernel = ...\n","  # compute the grayscale image \n","  grayscale = convolution(...)+convolution(...)+convolution(...)\n","  return grayscale\n","\n","# TODO - replace with path to the city.png image \n","image_path = '/content/gdrive/MyDrive/course_fmi_2022/Lab 5/city.png'\n","transf = transforms.ToTensor()\n","image = transf(Image.open(image_path))\n","\n","grayscale = rgb_2_grayscale(image)\n","\n","print('Original Image')\n","plt.imshow(image.permute(1,2,0))\n","plt.show()\n","print('Grayscale Image')\n","plt.imshow(grayscale, cmap='gray')\n","plt.show()"],"metadata":{"id":"iTfQm3jU3Eoe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Compute Blurred Grayscale Images\n"],"metadata":{"id":"qxFmWCNO2INy"}},{"cell_type":"markdown","source":["What if we also want to compute the blurred version of the image. So, we receive an RGB image and want to apply a convolution that returns us the blurred grayscale version of the image. \n","\n","In order to blur an image, we can simply perform an average of pixels in a local neighborhood. \n","\n","E.g. the following kernel can perform a blur over a $3\\times 3$ neighborhood\n","\n","$\\begin{bmatrix}\\frac{1}{9} & \\frac{1}{9} & \\frac{1}{9} \\\\ \\frac{1}{9} & \\frac{1}{9} & \\frac{1}{9} \\\\ \\frac{1}{9} & \\frac{1}{9} & \\frac{1}{9} \\end{bmatrix}$"],"metadata":{"id":"g-nCvOVtBm0U"}},{"cell_type":"code","source":["# TODO - implement the blur & grayscale conversion using convolution operations \n","# Note: this is a highly ineficient approach and is used solely for exemplification\n","def rgb_2_blurred_grayscale(image):\n","  # define the kernel that will perform the conversion\n","  kernel_r = torch.ones((5,5))* (1/9) * 0.299\n","  kernel_g = ... \n","  kernel_b = ... \n","  kernel = torch.cat(...) \n","  # compute the result \n","  blurred_grayscale = convolution(...)+convolution(...)+convolution(...)\n","  return blurred_grayscale\n","\n","# TODO - replace with path to the city.png image \n","image_path = '/content/gdrive/MyDrive/course_fmi_2022/Lab 5/city.png'\n","transf = transforms.ToTensor()\n","image = transf(Image.open(image_path))\n","\n","blurred_grayscale = rgb_2_blurred_grayscale(image)\n","\n","print('Original Image')\n","plt.imshow(image.permute(1,2,0))\n","plt.show()\n","print('Blurred Grayscale Image')\n","plt.imshow(blurred_grayscale, cmap='gray')\n","plt.show()\n","print('Grayscale Image')\n","plt.imshow(grayscale, cmap='gray')\n","plt.show()"],"metadata":{"id":"YAdwhN3s8CRe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <font color='ED1F24'>Part II: CNN Layers  \n","\n"],"metadata":{"id":"9RePzp3dTsET"}},{"cell_type":"markdown","source":["<div>\n","<img src=https://drive.google.com/uc?id=1PtO6mNFtm71ftrn2ea0huyvEk9PpmXSd width=\"750\"/>\n","<div>\n","\n","*Image Source: [FloydHub](https://blog.floydhub.com/building-your-first-convnet/)*"],"metadata":{"id":"CoFCMdztDT8R"}},{"cell_type":"markdown","source":["##Convolution Layer\n"],"metadata":{"id":"X_Uxh4P_QMiL"}},{"cell_type":"markdown","source":["\n","[torch.nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) - applies a 2D convolution over a single / multi-channel input signal\n","\n","\n","<div>\n","<img src=https://drive.google.com/uc?id=1BvNJZQqLIBHZC7d5Ojix8orIOYQ9fbSl width=\"1000\"/>\n","<div>\n","\n","Input size: $(N, C_{in}, H, W)$\n","\n","Output size: $(N, C_{out}, H, W)$\n","\n","$N$ - batch size / number of examples\n","\n","$(H, W)$ - image size\n","\n","$C_{in}$ - number of input channels (e.g. 3 for RGB images)\n","\n","$C_{out}$ - number of output channels ( the number of filters)\n","\n","The output of the layer:\n","\n","$out(N_i, {C_{out}}_{j})=bias({C_{out}}_{j}) + \\sum_{k=0}^{C_{in}-1}weight({C_{out}}_{j}, k)\\ast input(N_i, k)$"],"metadata":{"id":"N8s9uExcDeAF"}},{"cell_type":"markdown","source":["**Stride** - defines the kernel shift \n","<div>\n","<img src=https://drive.google.com/uc?id=18yxT_b-cJvZAwxi1FrssToqmOJyzfvPC width=\"200\"/>\n","<div>\n","\n","*Image Source: [GitHub](https://github.com/vdumoulin/conv_arithmetic)*\n","\n","**Dilation**\n","\n","<div>\n","<img src=https://drive.google.com/uc?id=1abJdd9TECdpoImtWW8MSxzioa-50znHB width=\"200\"/>\n","<div>\n","\n","*Image Source: [GitHub](https://github.com/vdumoulin/conv_arithmetic)*\n","\n","**Groups**\n","\n","<div>\n","<img src=https://drive.google.com/uc?id=1CuK7nOzuFgOdhBDf723PKQcrz1Hkr-gV width=\"1000\"/>\n","<div>\n","\n","*Image Source: [link](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215)*"],"metadata":{"id":"nuWnHbxQLKEW"}},{"cell_type":"markdown","source":["Now, let's play with a few convolutions"],"metadata":{"id":"WwTxB0VKFZzT"}},{"cell_type":"code","source":["conv_layer = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3)\n","print(conv_layer.weight.shape)\n","print(conv_layer.weight) \n","print(conv_layer.bias.shape)\n","print(conv_layer.bias)"],"metadata":{"id":"xLFnpiSuROHH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conv_layer = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(3,5))\n","print(conv_layer.weight.shape)\n","print(conv_layer.weight) \n","print(conv_layer.bias.shape)\n","print(conv_layer.bias)"],"metadata":{"id":"21PfBryISqir"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conv_layer = torch.nn.Conv2d(in_channels=3, out_channels=1, kernel_size=(3,3))\n","print(conv_layer.weight.shape)\n","print(conv_layer.weight) \n","print(conv_layer.bias.shape)\n","print(conv_layer.bias)"],"metadata":{"id":"-3gjuPpaSxCZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Try running the following configuration \n","# Is everything working fine? Can you explain why?\n","conv_layer = torch.nn.Conv2d(in_channels=3, out_channels=1, kernel_size=(3,5), groups=3)\n","print(conv_layer.weight.shape)\n","print(conv_layer.weight) \n","print(conv_layer.bias.shape)\n","print(conv_layer.bias)"],"metadata":{"id":"VP0_y57qS0LH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We can drop the bias term\n","conv_layer = torch.nn.Conv2d(in_channels=3, out_channels=3, kernel_size=(3,5), groups=3, bias=False)\n","print(conv_layer.weight.shape)\n","print(conv_layer.weight) \n","print(conv_layer.bias.shape)\n","print(conv_layer.bias)"],"metadata":{"id":"iEbD9wkDU6UP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Edge detection\n"],"metadata":{"id":"25oYP4_4Tnye"}},{"cell_type":"markdown","source":["Use torch.nn.Conv2d to implement the edge detector"],"metadata":{"id":"ZSa2sW8iR6lT"}},{"cell_type":"code","source":["def sobel_edges_with_conv2d(image):\n","  # define the horizontal edges kernel\n","  sobel_horizontal_kernel = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1, bias=False)\n","  # define the vertical edges kernel\n","  sobel_vertical_kernel = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1, bias=False)\n","\n","  kernel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n","  kernel_y = torch.tensor([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n","  with torch.no_grad():\n","    sobel_horizontal_kernel.weight.data.copy_(kernel_x)\n","    sobel_vertical_kernel.weight.data.copy_(kernel_y)\n","  \n","  # note that the conv2d layer receives a batch of N images \n","  edges_x = sobel_horizontal_kernel(image[None,None,:,:]).squeeze().detach()\n","  edges_y = sobel_vertical_kernel(image[None,None,:,:]).squeeze().detach()\n","  edges = torch.sqrt(torch.pow(edges_x,2)+torch.pow(edges_y,2))\n","  return edges, edges_x, edges_y\n","  \n","real_image_path = '/content/gdrive/MyDrive/course_fmi_2022/lab5_images/img_square_resize.jpg'\n","transf = transforms.ToTensor()\n","image = transf(Image.open(real_image_path).convert(mode='L'))[0]\n","\n","edges, edges_x, edges_y = sobel_edges_with_conv2d(image)\n","\n","print('Original Image')\n","plt.imshow(image, cmap='gray')\n","plt.show()\n","print('Horizontal Edges')\n","plt.imshow(edges_x, cmap='gray')\n","plt.show()\n","print('Vertical Edges')\n","plt.imshow(edges_y, cmap='gray')\n","plt.show()\n","print('Edges')\n","plt.imshow(edges, cmap='gray')\n","plt.show()\n"],"metadata":{"id":"grtqTAqxT28_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Pooling Layer\n"],"metadata":{"id":"zVS-bAulQOkC"}},{"cell_type":"markdown","source":["\n","<div>\n","<img src=https://drive.google.com/uc?id=1a8mTuEPXVXpcTnV08V3PjOIh7AYTlTme width=\"400\"/>\n","<div>\n","\n","*Image Source: [link](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)*"],"metadata":{"id":"Km8edy3lGPvp"}},{"cell_type":"markdown","source":["### Max Pooling\n","\n","\n","\n"],"metadata":{"id":"oq93p2-5bFXg"}},{"cell_type":"markdown","source":["[torch.nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)\n","\n","<div>\n","<img src=https://drive.google.com/uc?id=1uInwM4lPwuda5i_yxggSMYL9aLFhMJGi width=\"1000\"/>\n","<div>"],"metadata":{"id":"_4ydMxuvGVP_"}},{"cell_type":"code","source":["x = torch.rand((4,4))\n","print(x)\n","maxpool_layer = torch.nn.MaxPool2d(kernel_size=2)\n","x = x.unsqueeze(0).unsqueeze(0)\n","x = maxpool_layer(x)\n","print(x)"],"metadata":{"id":"uVQ7Dj4dc0rI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Average Pooling\n","\n"],"metadata":{"id":"Po95nesObHvF"}},{"cell_type":"markdown","source":["[torch.nn.AvgPool2d](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html#torch.nn.AvgPool2d)\n","\n","<div>\n","<img src=https://drive.google.com/uc?id=1fs2PhzwTIKZe9WHtP1nJ-tTLR5sD5bHC width=\"1000\"/>\n","<div>\n"],"metadata":{"id":"zMvjVneuGYVX"}},{"cell_type":"code","source":["x = torch.rand((4,4))\n","print(x)\n","avgpool_layer = torch.nn.AvgPool2d(kernel_size=2)\n","x = x.unsqueeze(0).unsqueeze(0)\n","x = avgpool_layer(x)\n","print(x)"],"metadata":{"id":"toKjE_LadQq4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <font color='ED1F24'> Part III: CNNs for Image Classification"],"metadata":{"id":"R_GdQRKegwZr"}},{"cell_type":"markdown","source":["## MNIST Dataset\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"Kv8HdjQDhEfH"}},{"cell_type":"markdown","source":["For this section we will use [The MNIST Database](http://yann.lecun.com/exdb/mnist/).\n","\n","\n","*   handwritten digits\n","*   size-normalized and centered in a fixed-size image ($28 \\times 28$)\n","\n","*   training set - 60 000 examples \n","*   test set - 10 000 examples "],"metadata":{"id":"88XpAyyMG2Og"}},{"cell_type":"code","source":["# download the MNIST dataset\n","!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n","!tar -zxvf MNIST.tar.gz"],"metadata":{"id":"6OI9gKEOeqFp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# we create a loader to iterate through the dataset\n","# https://pytorch.org/vision/stable/datasets.html\n","batch_size = 64\n","train_dataloader = torch.utils.data.DataLoader(\n","    datasets.MNIST('./', train=True, download=True,\n","                   transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                   ])),\n","    batch_size=batch_size, shuffle=True,drop_last=True)\n","\n","test_dataloader = torch.utils.data.DataLoader(\n","    datasets.MNIST('./', train=False, transform=transforms.Compose([\n","                       transforms.ToTensor(),\n","                   ])),\n","    batch_size=batch_size, shuffle=False,drop_last=True)\n","\n","first_train_batch_imgs, first_train_batch_labels = next(iter(train_dataloader))\n","print(first_train_batch_imgs.shape)\n","print(first_train_batch_labels.shape)\n","\n","f, axarr = plt.subplots(1,5)\n","for i in range(5):\n","  axarr[i].imshow(first_train_batch_imgs[i,0], cmap='gray')\n","print(f'Labels of the shown images: {first_train_batch_labels[:5]}')"],"metadata":{"id":"21h_Cwmfe0Io"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Image Classification with a Fully Connected Network\n","\n","We will try to solve the problem with a fully connected network first.\n","\n","\n","\n"],"metadata":{"id":"QwGMtjZAhJGf"}},{"cell_type":"code","source":["class FCNNClassifier(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.layer_1 = nn.Linear(in_features=1*28*28, out_features=500)\n","        self.output_layer = nn.Linear(in_features=500, out_features = 10)\n","        self.activation_fn = nn.ReLU()\n","\n","    def forward(self, x):\n","        # flatten the images\n","        x = x.view(x.shape[0], -1)\n","        x = self.activation_fn(self.layer_1(x))\n","        x = self.output_layer(x)\n","        return x"],"metadata":{"id":"MG2Z-QKFhxn2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(115)\n","fcnn_model = FCNNClassifier()\n","\n","num_params = 0\n","print(\"Model's parameters: \")\n","for n, p in fcnn_model.named_parameters():\n","    print('\\t', n, ': ', p.size())\n","    num_params += p.numel()\n","print(\"Number of model parameters: \", num_params)"],"metadata":{"id":"lXOeexhmuiuv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will define the training and validation loops."],"metadata":{"id":"QLIuGF--mLxq"}},{"cell_type":"code","source":["def train_epoch(model, train_dataloader, loss_crt, optimizer, device):\n","    \"\"\"\n","    model: Model object \n","    train_dataloader: DataLoader over the training dataset\n","    loss_crt: loss function object\n","    optimizer: Optimizer object\n","    device: torch.device('cpu) or torch.device('cuda')\n","\n","    The function returns: \n","     - the epoch training loss, which is an average over the individual batch\n","       losses\n","    \"\"\"\n","    model.train()\n","    epoch_loss = 0.0\n","    epoch_accuracy = 0.0\n","    num_batches = len(train_dataloader)\n","    \n","    for batch_idx, batch in tqdm(enumerate(train_dataloader)):\n","        # shape: batch_size x 1 x 28 x 28, batch_size x 1\n","        batch_img, batch_labels = batch\n","        \n","        # move data to GPU\n","        batch_img = batch_img.to(device)\n","        batch_labels = batch_labels.to(device)\n","        \n","        # initialize as zeros all the gradients of the model\n","        model.zero_grad()\n","\n","        # get predictions from the FORWARD pass \n","        # shape: batch_size x 10\n","        output = model(batch_img)\n","\n","        loss = loss_crt(output, batch_labels.squeeze())       \n","        loss_scalar = loss.item()\n","\n","        # BACKPROPAGATE the gradients\n","        loss.backward()\n","        # use the gradients to OPTIMISE the model\n","        optimizer.step()\n","        \n","        epoch_loss += loss_scalar\n","\n","        pred = output.argmax(dim=1, keepdim=True)\n","        epoch_accuracy += pred.eq(batch_labels.view_as(pred)).float().mean().item()\n","        \n","    epoch_loss = epoch_loss/num_batches\n","    epoch_accuracy = 100. * epoch_accuracy/num_batches\n","    return epoch_loss, epoch_accuracy\n","\n","def eval_epoch(model, val_dataloader, loss_crt, device):\n","    \"\"\"\n","    model: Model object \n","    val_dataloader: DataLoader over the validation dataset\n","    loss_crt: loss function object\n","    device: torch.device('cpu) or torch.device('cuda')\n","\n","    The function returns: \n","     - the epoch validation loss, which is an average over the individual batch\n","       losses\n","    \"\"\"\n","    model.eval()\n","    epoch_loss = 0.0\n","    epoch_accuracy = 0.0\n","    num_batches = len(val_dataloader)\n","    with torch.no_grad():\n","        for batch_idx, batch in tqdm(enumerate(val_dataloader)):\n","            # shape: batch_size x 3 x 28 x 28, batch_size x 1\n","            batch_img, batch_labels = batch\n","            current_batch_size = batch_img.size(0)\n","\n","            # move data to GPU\n","            batch_img = batch_img.to(device)\n","            batch_labels = batch_labels.to(device)\n"," \n","            # batch_size x 10\n","            output = model(batch_img)\n","\n","            loss = loss_crt(output, batch_labels.squeeze())\n","            loss_scalar = loss.item()\n","\n","            epoch_loss += loss_scalar\n","\n","            pred = output.argmax(dim=1, keepdim=True)\n","            epoch_accuracy += pred.eq(batch_labels.view_as(pred)).float().mean().item()\n","\n","    epoch_loss = epoch_loss/num_batches\n","    epoch_accuracy = 100. * epoch_accuracy/num_batches\n","    return epoch_loss, epoch_accuracy"],"metadata":{"id":"Jfp4mAOkl-Q6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# move the model to GPU (when available)\n","fcnn_model.to(device)\n","\n","# create a SGD optimizer\n","optimizer = torch.optim.SGD(fcnn_model.parameters(), lr=0.01, momentum=0.9)\n","\n","# set up loss function\n","loss_criterion = nn.CrossEntropyLoss()\n","\n","num_epochs = 10\n","train_losses = []\n","train_accuracies = []\n","val_losses = []\n","val_accuracies = []\n","for epoch in range(1, num_epochs+1):\n","  train_loss, train_accuracy = train_epoch(fcnn_model, train_dataloader, loss_criterion, optimizer, device)\n","  val_loss, val_accuracy = eval_epoch(fcnn_model, test_dataloader, loss_criterion, device)\n","  train_losses.append(train_loss)\n","  val_losses.append(val_loss)\n","  train_accuracies.append(train_accuracy)\n","  val_accuracies.append(val_accuracy)\n","  print('\\nEpoch %d'%(epoch))\n","  print('train loss: %10.8f, accuracy: %10.8f'%(train_loss, train_accuracy))\n","  print('val loss: %10.8f, accuracy: %10.8f'%(train_loss, train_accuracy))\n"],"metadata":{"id":"INn8l1PVnWQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot loss & accuracy\n","plt.figure()\n","plt.plot(train_losses, label='train_loss', color='red')\n","plt.plot(val_losses, label='val_loss', color='blue')\n","plt.legend()\n","plt.show()\n","plt.plot(train_accuracies, label='train_accuracy', color='red')\n","plt.plot(val_accuracies, label='val_accuracy', color='blue')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"yU2-HasF21o0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Image Classification with a Convolutional Neural Network\n","\n","\n","\n"],"metadata":{"id":"Hw0qXtE1hZum"}},{"cell_type":"markdown","source":["Now, we will solve the same classification problem using a CNN model. \n","\n","The architecture:\n","\n","*   Conv Layer: 20 filters, kernel size: 5x5, stride: 1\n","*   ReLU\n","*   Max Pool Layer: kernel size: 2x2, stride: 2\n","*   Conv Layer: 50 filters, kernel size: 5x5, stride:1\n","*   ReLU \n","*   Max Pool Layer: kernel size: 2x2, stride: 2\n","*   Fully Connected Layer: 500 neurons\n","*   ReLU\n","*   Fully Connected Layer: 10 neurons "],"metadata":{"id":"dIA78-knIpUN"}},{"cell_type":"code","source":["# implement the above architecture \n","class CNNClassifier(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(...)\n","        self.conv2 = nn.Conv2d(...)\n","        # note that the maps are flattened before this layer\n","        # you need to infer the input size of the fully connected layer\n","        self.fc1 = nn.Linear(...)\n","        self.fc2 = nn.Linear(...)\n","        self.activation_fn = ...\n","        self.pool = nn.MaxPool2d(...)\n","\n","    def forward(self, x):\n","        x = ...\n","        return x"],"metadata":{"id":"OuVS-wQY9KA2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(115)\n","cnn_model = CNNClassifier()\n","\n","num_params = 0\n","print(\"Model's parameters: \")\n","for n, p in cnn_model.named_parameters():\n","    print('\\t', n, ': ', p.size())\n","    num_params += p.numel()\n","print(\"Number of model parameters: \", num_params)"],"metadata":{"id":"7Z_z1EUGAOxr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# move the model to GPU (when available)\n","cnn_model.to(device)\n","\n","# create a SGD optimizer\n","optimizer = torch.optim.SGD(cnn_model.parameters(), lr=0.01, momentum=0.9)\n","\n","# set up loss function\n","loss_criterion = nn.CrossEntropyLoss()\n","\n","num_epochs = 10\n","train_losses = []\n","train_accuracies = []\n","val_losses = []\n","val_accuracies = []\n","for epoch in range(1, num_epochs+1):\n","  train_loss, train_accuracy = train_epoch(cnn_model, train_dataloader, loss_criterion, optimizer, device)\n","  val_loss, val_accuracy = eval_epoch(cnn_model, test_dataloader, loss_criterion, device)\n","  train_losses.append(train_loss)\n","  val_losses.append(val_loss)\n","  train_accuracies.append(train_accuracy)\n","  val_accuracies.append(val_accuracy)\n","  print('\\nEpoch %d'%(epoch))\n","  print('train loss: %10.8f, accuracy: %10.8f'%(train_loss, train_accuracy))\n","  print('val loss: %10.8f, accuracy: %10.8f'%(val_loss, val_accuracy))"],"metadata":{"id":"Y_76fYAKH2Ac"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot loss & accuracy\n","plt.figure()\n","plt.plot(train_losses, label='train_loss', color='red')\n","plt.plot(val_losses, label='val_loss', color='blue')\n","plt.legend()\n","plt.show()\n","plt.plot(train_accuracies, label='train_accuracy', color='red')\n","plt.plot(val_accuracies, label='val_accuracy', color='blue')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"n92ExeuzICx8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <font color='ED1F24'> Part IV: Transfer Learning "],"metadata":{"id":"ukqcASGZg7T-"}},{"cell_type":"markdown","source":["\n","<div>\n","<img src=https://drive.google.com/uc?id=1Lx_VHuOyjariEPPrB6iahV0_30mrXLB7 width=\"700\"/>\n","<div>"],"metadata":{"id":"XFsj8v2-UnhP"}},{"cell_type":"markdown","source":["**How**:\n","\n","\n","*   finetuning - start from the pretrained weights, but the whole model is retrained.\n","*   feature extraction - use the pretrained model solely for feature extraction and we train a classifier on top of those features.\n","\n","In this laboratory we will implement the feature extraction approach.\n","\n"],"metadata":{"id":"zOYjBKyRgf8l"}},{"cell_type":"markdown","source":["## ResNet Architecture\n"],"metadata":{"id":"qYIrVFR6o13O"}},{"cell_type":"markdown","source":["\n","<div>\n","<img src=https://drive.google.com/uc?id=1QX_AYUEYYzQcBLVGaRHfXj24ttmHFdHN width=\"1000\"/>\n","<div>\n","\n","*Image Source: He et al. Deep Residual Learning for Image Recognition [paper](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)*\n","\n","\n","Main advantages\n","*   train deeper models\n","*   easily optimise by allowing direct paths between lower and upper levels\n","\n","More detailes regarding the ResNet architecture will be discussed in next laboratory and during the lectures. For the moment, we will treat it as a black box."],"metadata":{"id":"OdqecQahOmLl"}},{"cell_type":"markdown","source":["## Pretrained ResNet\n","\n","Multiple model definitions along with their pretrained weights are available in the [torchvision.models](https://pytorch.org/vision/0.8/models.html). The models have been pretrained on the 1000-class ImageNet dataset.\n","\n","The model inputs are mini-batches of 3-channel images of shape $(3\\times height \\times width)$. $height$ and $width$ are expected to be at least $224$\n","\n","The input images should be in range $[0,1]$, normalized using:\n","\n","$mean=[0.485, 0.456, 0.406]$\n","\n","$std=[0.229, 0.224, 0.225]$\n","\n"],"metadata":{"id":"8_sz_iBvo4cc"}},{"cell_type":"code","source":["import torchvision.models as models \n","# load the ResNet-18 model, with randomly initialized weights \n","resnet18_random = models.resnet18(pretrained=False)\n","# load the ResNet-18 model, with weights pretrained on ImageNet \n","resnet18_pretrained = models.resnet18(pretrained=True)\n","\n","num_params = 0\n","print(\"Model's parameters: \")\n","for n, p in resnet18_pretrained.named_parameters():\n","    print('\\t', n, ': ', p.size())\n","    num_params += p.numel()\n","print(\"Number of model parameters: \", num_params)"],"metadata":{"id":"MYN9yAYAONsY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO - apply the ResNet model over a randomly generated image (224 x 224) & check the output dimension \n","...\n","\n"],"metadata":{"id":"M0upjl_uQjxt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO - apply the ResNet model over a real image, after resizing it to 224 x 224\n","# do not forget to apply the required transformations over the image\n","# try resizing the image to 28 x 28 & see what happens \n","# TODO - replace with path to the city.png image \n","image_path = '/content/gdrive/MyDrive/course_fmi_2022/Lab 5/city.png'\n","# define transformations to be applied over the image\n","transf = ...\n","image = ...\n","# apply ResNet\n","res = resnet18_pretrained(...)\n","print(res.shape)\n"],"metadata":{"id":"SQ4bYdIQRJsl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Cats vs. Dogs Dataset\n","\n","\n","\n"],"metadata":{"id":"53sKSW-5esAp"}},{"cell_type":"markdown","source":["We will work on the Cats vs. Dogs Dataset introduced in Laboratory 3\n","\n","*   classify images based on their content\n","*   images containing cats or dogs $⇒$ 2 classes\n","*   training set - 2000 examples \n","*   test set - 1000 examples"],"metadata":{"id":"LM7_-OsgPrlK"}},{"cell_type":"code","source":["import zipfile\n","import os\n","\n","# Download the samples\n","!wget --no-check-certificate \\\n","https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n","-O /tmp/cats_and_dogs_filtered.zip\n","\n","# Extract the data\n","local_zip = '/tmp/cats_and_dogs_filtered.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/tmp')\n","zip_ref.close()\n","\n","# set up train and validation dirs\n","base_dir = '/tmp/cats_and_dogs_filtered'\n","train_dir = os.path.join(base_dir, 'train')\n","validation_dir = os.path.join(base_dir, 'validation')"],"metadata":{"id":"ZyHC657xftfE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CatsAndDogsDataset(Dataset):\n","  def __init__(self, data_dir: str, transform=None):\n","    # dir structure:\n","    # /\n","    #  /cats/\n","    #        cat.0.jpg\n","    #        cat.1.jpg\n","    #        ...\n","    #  /dogs/\n","    #        dog.0.jpg  \n","    #        dog.1.jpg\n","    #        ...           \n","    #  \n","    self.data_dir = data_dir\n","\n","    # directory containing cat pictures\n","    self.cats_dir = os.path.join(self.data_dir, 'cats')\n","\n","    # directory containing dog pictures\n","    self.dogs_dir = os.path.join(self.data_dir, 'dogs')\n","\n","    self.cat_fnames = [os.path.join(self.cats_dir, fname) \\\n","                        for fname in os.listdir(self.cats_dir)]\n","    self.dog_fnames = [os.path.join(self.dogs_dir, fname) \\\n","                        for fname in os.listdir(self.dogs_dir)]\n","\n","    self.fnames = self.cat_fnames + self.dog_fnames\n","\n","    self.labels = len(self.cat_fnames) * [0] + len(self.dog_fnames) * [1]\n","\n","    # TODO - implement the transformations \n","    #      - resize images to 224 x 224 \n","    #      - normalize images \n","  \n","    self.image_transforms = ...\n","\n","  def __getitem__(self, index):\n","    fname = self.fnames[index]\n","    img_obj = Image.open(fname)\n","    img_tensor = self.image_transforms(img_obj)\n","    \n","    # retrieve the image's label and store it into a Tensor\n","    label = self.labels[index]\n","    label_tensor = torch.tensor([label])\n","\n","    return img_tensor, label_tensor\n","  \n","  def __len__(self):\n","    return len(self.fnames)"],"metadata":{"id":"-knDSfpRfzfA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get train & validation datasets \n","train_dataset = CatsAndDogsDataset(data_dir=train_dir)\n","validation_dataset = CatsAndDogsDataset(data_dir=validation_dir)\n","# instantiate the dataloaders\n","batch_size = 64\n","train_dataloader = DataLoader(\n","    dataset=train_dataset, \n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","validation_dataloader = DataLoader(\n","    dataset=validation_dataset, \n","    batch_size=batch_size\n",")\n","\n","# visualize few examples\n","train_batch_imgs, train_batch_labels = next(iter(train_dataloader))\n","print(train_batch_imgs.shape)\n","print(train_batch_labels.shape)\n","\n","f, axarr = plt.subplots(1,5)\n","for i in range(5):\n","  axarr[i].imshow(train_batch_imgs[i].permute(1,2,0))\n","print(f'Labels of the shown images: {train_batch_labels[:5]}')"],"metadata":{"id":"MxvQsiKng6NP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Feature extraction\n","\n","We use the pretrained model solely for feature extraction and we train a classifier on top of those features."],"metadata":{"id":"8MeFGnl2U_Rk"}},{"cell_type":"code","source":["# function counting the number of parameters and the number of trainable parameters of a model \n","# optionally, it will also display the layers\n","def check_model_parameters(model, display_layers=False):\n","  num_params = 0\n","  num_trainable_params = 0\n","  if display_layers==True:\n","    print(\"Model's parameters: \")\n","  for n, p in resnet18_pretrained.named_parameters():\n","      if display_layers == True:\n","        print('\\t', n, ': ', p.size())\n","      num_params += p.numel()\n","      if p.requires_grad:\n","        num_trainable_params += p.numel()\n","  print(\"Number of model parameters: \", num_params)\n","  print(\"Number of trainable parameters: \", num_trainable_params)"],"metadata":{"id":"GYhM8Y_yXdGM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# freeze the model parameters \n","import torchvision.models as models \n","# load the ResNet-18 model, with weights pretrained on ImageNet \n","resnet18_pretrained = models.resnet18(pretrained=True)\n","\n","# check the number of parameters and the number of trainable parameters\n","check_model_parameters(resnet18_pretrained, display_layers=False)\n","\n","# freeze all the layers\n","for param in resnet18_pretrained.parameters():\n","  param.requires_grad = False \n","\n","# check the number of parameters and the number of trainable parameters\n","check_model_parameters(resnet18_pretrained, display_layers=False)\n"],"metadata":{"id":"drNo5CVBW2Tb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(115)\n","# TODO - change the last layer of the model to adapt it for our task\n","# Hint - look at the layers of the model, you need to change the last one s.t. the number of output classes is 2, instead of 1000\n","#      - this last layer is a simple linear layer\n","...\n","\n","check_model_parameters(resnet18_pretrained, display_layers=True)"],"metadata":{"id":"8kylTKHBZCqL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_epoch(model, train_dataloader, loss_crt, optimizer, device):\n","    \"\"\"\n","    model: Model object \n","    train_dataloader: DataLoader over the training dataset\n","    loss_crt: loss function object\n","    optimizer: Optimizer object\n","    device: torch.device('cpu) or torch.device('cuda')\n","\n","    The function returns: \n","     - the epoch training loss, which is an average over the individual batch\n","       losses\n","    \"\"\"\n","    model.train()\n","    epoch_loss = 0.0\n","    epoch_accuracy = 0.0\n","    num_batches = len(train_dataloader)\n","    \n","    for batch_idx, batch in tqdm(enumerate(train_dataloader)):\n","        # shape: batch_size x 1 x 28 x 28, batch_size x 1\n","        batch_img, batch_labels = batch\n","        \n","        # move data to GPU\n","        batch_img = batch_img.to(device)\n","        batch_labels = batch_labels.to(device)\n","        \n","        # initialize as zeros all the gradients of the model\n","        model.zero_grad()\n","\n","        # get predictions from the FORWARD pass \n","        # shape: batch_size x 10\n","        output = model(batch_img)\n","\n","        loss = loss_crt(output, batch_labels.squeeze())       \n","        loss_scalar = loss.item()\n","\n","        # BACKPROPAGATE the gradients\n","        loss.backward()\n","        # use the gradients to OPTIMISE the model\n","        optimizer.step()\n","        \n","        epoch_loss += loss_scalar\n","\n","        pred = output.argmax(dim=1, keepdim=True)\n","        epoch_accuracy += pred.eq(batch_labels.view_as(pred)).float().mean().item()\n","        \n","    epoch_loss = epoch_loss/num_batches\n","    epoch_accuracy = 100. * epoch_accuracy/num_batches\n","    return epoch_loss, epoch_accuracy\n","\n","def eval_epoch(model, val_dataloader, loss_crt, device):\n","    \"\"\"\n","    model: Model object \n","    val_dataloader: DataLoader over the validation dataset\n","    loss_crt: loss function object\n","    device: torch.device('cpu) or torch.device('cuda')\n","\n","    The function returns: \n","     - the epoch validation loss, which is an average over the individual batch\n","       losses\n","    \"\"\"\n","    model.eval()\n","    epoch_loss = 0.0\n","    epoch_accuracy = 0.0\n","    num_batches = len(val_dataloader)\n","    with torch.no_grad():\n","        for batch_idx, batch in tqdm(enumerate(val_dataloader)):\n","            # shape: batch_size x 3 x 28 x 28, batch_size x 1\n","            batch_img, batch_labels = batch\n","            current_batch_size = batch_img.size(0)\n","\n","            # move data to GPU\n","            batch_img = batch_img.to(device)\n","            batch_labels = batch_labels.to(device)\n"," \n","            # batch_size x 10\n","            output = model(batch_img)\n","\n","            loss = loss_crt(output, batch_labels.squeeze())\n","            loss_scalar = loss.item()\n","\n","            epoch_loss += loss_scalar\n","\n","            pred = output.argmax(dim=1, keepdim=True)\n","            epoch_accuracy += pred.eq(batch_labels.view_as(pred)).float().mean().item()\n","\n","    epoch_loss = epoch_loss/num_batches\n","    epoch_accuracy = 100. * epoch_accuracy/num_batches\n","    return epoch_loss, epoch_accuracy"],"metadata":{"id":"QdjUqwshj-rs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["resnet18_pretrained.to(device)\n","\n","# create a SGD optimizer\n","optimizer = torch.optim.SGD(resnet18_pretrained.parameters(), lr=0.01, momentum=0.9)\n","\n","# set up loss function\n","loss_criterion = nn.CrossEntropyLoss()\n","\n","# evaluate the initial model \n","val_loss, val_accuracy = eval_epoch(resnet18_pretrained, validation_dataloader, loss_criterion, device)\n","print('Validation performance before finetuning -- loss: %10.8f, accuracy: %10.8f'%(val_loss, val_accuracy))\n","\n","# finetune the model \n","num_epochs = 2\n","train_losses = []\n","train_accuracies = []\n","val_losses = []\n","val_accuracies = []\n","for epoch in range(1, num_epochs+1):\n","  train_loss, train_accuracy = train_epoch(resnet18_pretrained, train_dataloader, loss_criterion, optimizer, device)\n","  val_loss, val_accuracy = eval_epoch(resnet18_pretrained, validation_dataloader, loss_criterion, device)\n","  train_losses.append(train_loss)\n","  val_losses.append(val_loss)\n","  train_accuracies.append(train_accuracy)\n","  val_accuracies.append(val_accuracy)\n","  print('\\nEpoch %d'%(epoch))\n","  print('train loss: %10.8f, accuracy: %10.8f'%(train_loss, train_accuracy))\n","  print('val loss: %10.8f, accuracy: %10.8f'%(val_loss, val_accuracy))"],"metadata":{"id":"xEqHUtMTjeef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO - train the resnet architecture from scratch and compare the obtained results \n","#      - we will employ the same training strategy \n","torch.manual_seed(115)\n","...S"],"metadata":{"id":"2ko_IHt_SDUD"},"execution_count":null,"outputs":[]}]}