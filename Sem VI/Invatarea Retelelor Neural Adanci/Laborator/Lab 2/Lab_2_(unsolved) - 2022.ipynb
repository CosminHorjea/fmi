{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab_2_(unsolved) - 2022.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"VT-N4Kx-ZbQu"},"source":["<font size=25>Laboratory 2 summary</font>\n","\n","In this lab you will:\n","* Learn how the `torch.autograd` package tracks the computation graph and performs backpropagation\n","* Solve Linear Regression by training a Single Layer Perceptron with Gradient Descent, using:\n","  - layer written without help from ```torch.nn``` [layers](https://pytorch.org/docs/stable/nn.html)\n","  - optimizer written from scratch\n","  - loss function written from scratch\n","* train neural networks using:\n"," - [layers](https://pytorch.org/docs/stable/nn.html#linear-layers) from the ```torch.nn``` package in PyTorch\n"," - [optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD) defined in PyTorch\n"," - [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions) defined in PyTorch"]},{"cell_type":"markdown","source":["# **Part I: Autograd**"],"metadata":{"id":"ekXl_xOApW7_"}},{"cell_type":"markdown","metadata":{"id":"_VFxGzPYv4xl"},"source":["## Computational Graph\n","The loss function (which contains the neural network output) can be seen as a very complex function. It can be decomposed into basic operations and viewed as a directed graph, where internal nodes are operations (functions) and the edges contain the tensors. \n","\n","|![Computational Graph](https://i.ibb.co/xX1kYvy/computational-graph.png)|\n","|:--:|\n","| Computational graph for simple linear neural network and SVM loss + regularization. [Source: CS231, lecture 4, slide 38](http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture04.pdf) |\n","\n","To optimize $W$ using gradient descent, we need to compute the gradient of the loss $L_i$ with respect to $W$: $\\nabla_{W}L_i$\n","\n"]},{"cell_type":"markdown","source":["## How to compute gradient\n"," - derive manually (difficult)\n"," - let software automatically compute the gradient using the computational graph and chain rule\n","\n","|![Computational Graph](https://i.ibb.co/zr7TV4H/computational-graph.png)|\n","|:--:|\n","| Example of a simple computational graph. Autograd computes df/dx, df/dy, df/dz. [Source: CS231, lecture 4](http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture04.pdf) |\n","\n","This procedure for finding derivatives of complex functions is called [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) (or autodiff). Backpropagation is a particular case of automatic differentiation (called reverse mode automatic differentiation), in which gradients are computed from the root to the leaves.\n","\n","\n","The [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) package provides automatic differentiation for all operations on Tensors. It is a *define-by-run* framework (the computational graph is *dynamic*), which means that your backprop is defined by how your code is run, and that every single iteration can be different (as opposed to TensorFlow, where you first define the graph and then run it - a *static graph*).\n","\n","To track operations on a Tensor `x`, we need to set its attribute `requires_grad` to `True`. Calling `.backward()` on any output Tensor `y` will compute the gradients of `y` with respect to all leaves in the graph. The gradient for tensor `x` will be accumulated into the `.grad` attribute.\n","\n","Thereâ€™s one more class which is very important for autograd implementation - a `Function`. `Tensor` and `Function` are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a `.grad_fn` attribute that references a Function that has created the Tensor (except for Tensors created by the user - their `grad_fn` is `None`).\n","\n","*Ref: [autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)*"],"metadata":{"id":"B0GwhfMSpg3h"}},{"cell_type":"markdown","metadata":{"id":"aJmk5NuSeiTj"},"source":["## Viewing the computational graph\n","\n","We can use `torchviz` to visualise the computation history. It displays the computation graph, coloring the leaf nodes in blue, the root (the element whose graph we are plotting in green), and all other intermediate operations in grey.\n","\n","First we would need to install and import it in this notebook:"]},{"cell_type":"code","metadata":{"id":"MiCa79zYdZXY"},"source":["!pip install torchviz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U6JPnw8SYxW8"},"source":["from functools import partial\n","from IPython.display import HTML\n","import math\n","import matplotlib.pyplot as plt\n","from matplotlib import animation, rc\n","import numpy as np\n","from sklearn.datasets import make_regression\n","from sklearn.linear_model import LinearRegression\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchviz\n","from typing import Iterator"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eS8uYE5NfElK"},"source":["\n","Let us define a simple Tensor, specifying the `requires_grad` parameter, and then draw the computation graph:"]},{"cell_type":"code","metadata":{"id":"jGXw1Z_5yKp2"},"source":["a = torch.ones(2, 2, requires_grad=True)\n","print(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v6cA_lzHyvpX"},"source":["b = torch.zeros(2, 2, requires_grad=True)\n","c = a + b\n","print(\"b = \", b)\n","print(\"c = \", c)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ivSzB2Hyn447"},"source":["Notice how the `grad_fn` attribute is now populated with `AddBackward0`. This means that `autograd` knows that the tensor `c` results from the addition operation between tensor `a` and `b`. To compute the gradients, autograd will traverse the computational graph from the root node (in this case, `c`) to its leaves (`a` and `b`).\n","\n","Let's visualize the graph:"]},{"cell_type":"code","metadata":{"id":"1ASfvD_nn2pS"},"source":["torchviz.make_dot(c, params={'a':a, 'b':b, 'c':c})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bA-Nsl_kfXk4"},"source":["Note that, had we not specified `requires_grad`, the computation history would not have been tracked."]},{"cell_type":"code","metadata":{"id":"SLf4czA7gTvQ"},"source":["a = torch.ones(2, 2)\n","b = torch.zeros(2, 2)\n","c = a + b\n","print(\"c = \", c)\n","torchviz.make_dot(c, params={'a':a, 'b':b, 'c':c})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GCi8pdbrgUOa"},"source":["Let's look at another example, involving more tensors:"]},{"cell_type":"code","metadata":{"id":"Xn4tcEFCfne3"},"source":["a = torch.randn(5, 2, requires_grad=True)\n","b = torch.randn(2, 7, requires_grad=True)\n","c = torch.randn(5, 7, requires_grad=True)\n","\n","x = torch.tanh(a @ b + c).sum()\n","\n","print(x)\n","torchviz.make_dot(x, params={'a':a, 'b':b, 'c':c})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BTyxX2dohsIq"},"source":["## Computing the gradient of the result with respect to all tensors in the graph\n","Let's consider a simple dot product multiplication\n"," $y = \\textbf{w}^T \\textbf{x}$ and plot it's computational graph:"]},{"cell_type":"code","metadata":{"id":"DxGtfXSqywyL"},"source":["w = torch.rand([3,1], requires_grad=True)\n","x = torch.tensor([[10.0, 11.0, 12.0]])\n","y = x @ w # equivalent to y = torch.matmul(w, x)\n","print(f'w = {w}')\n","print(f'x = {x}')\n","print(f'y = {y}')\n","torchviz.make_dot(y, params={'w':w, 'x':x, 'y': y})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p37w6dzZtIRJ"},"source":["We compute the gradient of y with respect to **w**: $\\nabla_\n","\\textbf{w} y = [\\frac{\\partial y}{\\partial w_1}, \\frac{\\partial y}{\\partial w_2}, \\frac{\\partial y}{\\partial w_3}] = [x_1, x_2, x_3] = \\textbf{x}$\n","\n","To do this in PyTorch, we simply call `backward()` on Tensor `y`:"]},{"cell_type":"code","metadata":{"id":"Rw6jmkw9zNFe"},"source":["y.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qCE4Xk4rLrrC"},"source":["The gradient of y with respect to **w** will be stored in `w.grad`:"]},{"cell_type":"code","metadata":{"id":"H4aaYq4jLuyh"},"source":["print(\"Gradient of y wrt w = \\n\", w.grad)\n","print(\"Gradient of y wrt x = \\n\", x.grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EKoFoy6MLzyx"},"source":["What is the gradient of y with respect to **x** and why was it not stored in `x.grad`?"]},{"cell_type":"markdown","metadata":{"id":"kRQXufAFiFZS"},"source":["**Important:** Calling `.backward()` erases the forward graph by default. Calling `.backward()` again will raise an error."]},{"cell_type":"code","metadata":{"id":"EgbCrW48zvI6"},"source":["try:\n","    y.backward()\n","except RuntimeError as e:\n","    print(e)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4ZtfWU440UTr"},"source":["## Exercise 1: Linear regression with autograd"]},{"cell_type":"markdown","metadata":{"id":"YwAR6tVpz9xK"},"source":["Recall the linear regression exercise - in our previous lab we've trained our model by computing the closed-form solution. Let's do a linear regression using Gradient Descent."]},{"cell_type":"markdown","metadata":{"id":"0MA5Jy4w6eWW"},"source":["We'd like to find the global minimum of the error function $E(w)$ - i.e. we want to find a *weight vector $w$* which minimizes $E(w)^{(*)}$.\n","\n","Let $t$ be our iteration step. We want to perform *Gradient Descent* on the loss surface - the simplest approach to using gradient information is to update the weights by making small steps in the direction of the negative gradient (see Chapter 5.2 - Bishop) [[1]]: \n","\n","$$w^{(t+1)}=w^{(t)} - \\eta\\nabla_w E(w^{(t)})$$\n","\n","The $\\eta$ parameter is known as the *learning rate* and it represents the magnitude of the gradient step.\n","\n","We're going to use the *Mean Squared Error (MSE)* loss for the Linear Regression. Recall that the MSE Loss is defined as follows:\n","\n","$$MSE(\\textbf{y}, \\hat{\\textbf{y}}) = \\frac{1}{n} \\sum_{i=0}^{n} (y_i - \\hat{y_i})^2 $$\n","\n","where $n$ is the dataset length, $\\textbf{y}$ is the true label vector and $\\hat{\\textbf{y}}$ is the vector containing the predictions.\n","\n","(\\*): *In the context of Neural Networks, this won't be always possible (or desirable - recall the polynomial that was overfitting our data in the previous lab). The loss surface of a Neural Network is a lot more complex - there could be saddle points, parts of the surface with almost no gradients or highly irregular neighbours. We'll (usually) find a \"good enough\" local minima, improving our chances of finding it by using various optimization tehniques.*\n","\n","[1]: http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf"]},{"cell_type":"markdown","metadata":{"id":"ZAICYqjNsFf0"},"source":["**TODO:**\n","\n","1) Implement the `__call__()` method of the `MSE` class. This method will compute the MSE Loss between predictions in `y` and ground-truth values in `target`\n","\n","2) Implement the `forward()` method for the `GDLinearRegression` class. This method will predict our points w.r.t. the weights and biases.\n","\n","3) Impement the `step()` method for the `GD` optimizer class. This method will perform a gradient descent update to the parameters.\n","\n","4) Implement the `train()` routine:\n","\n","- Make a prediction with your `GDLinearRegression` model.\n","- Calculate the MSE Loss using the `MSE` object.\n","- Calculate the gradients by using `.backward()` on the loss.\n","- Do an optimizer `.step()` and reset the gradients.\n"]},{"cell_type":"markdown","source":["### Implement loss function\n","\n","TODO: Implement the `__call__()` method of the `MSE` class below:"],"metadata":{"id":"wGCwN-6J_keI"}},{"cell_type":"code","metadata":{"id":"LgLn541Nw8m1"},"source":["class MSE():\n","  \"\"\"The Mean Squared Error loss\"\"\"\n","  \n","  def __call__(self, y: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    TODO: compute the MSE between predictions in tensor y and ground truth \n","    values in tensor target. Both tensors are unidimensional.\n","    y: tensor of size N containing the predictions \n","    target: tensor of size N containing the ground-truth values\n","    \"\"\"\n","    mse = ... #TODO\n","    return mse"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = torch.tensor([1, 2, 3])\n","target = torch.tensor([2, 3, 4])\n","mse = (y - target) ** 2 / y.shape[0]\n","print(y.shape[0])"],"metadata":{"id":"MoF65G_o9erN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's test our loss function."],"metadata":{"id":"rfST02xHAJY2"}},{"cell_type":"code","source":["loss_criterion = MSE()\n","y = torch.tensor([1.0, 2, 3])\n","target = torch.tensor([2.0, 3, 4])\n","mse_loss = loss_criterion(y, target)\n","print(\"MSE loss = \", mse_loss)"],"metadata":{"id":"ZG1LpfpzAWxJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note:** Unless we design a very particular loss function, we do not write them from scratch but use the [loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) defined in the `torch.nn` package."],"metadata":{"id":"87oyS27ZBExr"}},{"cell_type":"markdown","source":["### Implement model"],"metadata":{"id":"-wVzU2KMCKHk"}},{"cell_type":"markdown","metadata":{"id":"7dTU7B508cns"},"source":["All models written in PyTorch need to subclass the [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=module#torch.nn.Module) and implement the following methods:\n","1. `__init__` (the initialiser), where we initialise layers and activation functions\n","2. `forward`, which implements the feedforward step \n","\n","The constructor is already implemented. It initializes the model with the two parameters (weight and bias). The `w` and `b` Tensors are wrapped in the `nn.Parameter` class in order to be recognized as parameters by the `nn.Module`.\n","\n","**TODO**: Implement the `forward` method:"]},{"cell_type":"code","metadata":{"id":"hOd2nj0_xqsR"},"source":["class GDLinearRegression(nn.Module):\n","  \"\"\"A simple Linear Regression model\"\"\"\n","\n","  def __init__(self):\n","    super().__init__()\n","    # We initialize our model with random weights\n","    # nn.Parameter is just a wrapper over Tensor that tells `nn.Module` to \n","    # register the object as a parameter. It creates a Tensor with\n","    # requires_grad=True by default\n","    self.w = nn.Parameter(torch.randn(1))\n","    self.b = nn.Parameter(torch.randn(1))\n","\n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    TODO: implement feedforwad call for a simple linear regression:\n","      y = wx + b\n","    Arguments: x is tensor of size (num_examples x 1)\n","    \"\"\"\n","    #TODO\n","    #y = ...\n","    \n","    return y\n","\n","  # After each Gradient Descent step we should reset the gradients, \n","  # otherwise they accumulate.\n","\n","  # NOTE: this function is already provided by `nn.Module`\n","  # https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.zero_grad\n","  # However, we overwrite it below more simply so you understand what happens\n","  # behind the scenes\n","  def zero_grad(self):\n","    self.w.grad.zero_()\n","    self.b.grad.zero_()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `nn.Module` class provides us with the `.parameters()` method, which conveniently return a list of all the model parameters (needed for optimization)."],"metadata":{"id":"1khkMZSbGn6X"}},{"cell_type":"code","source":["model = GDLinearRegression()\n","print(list(model.parameters()))"],"metadata":{"id":"86fqYfB5nBeh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There is also the `.named_parameters()` method, which returns a dictionary that maps a parameter's name to its actual Tensor."],"metadata":{"id":"C9NtE9IfR1-x"}},{"cell_type":"code","source":["for name, p in model.named_parameters():\n","    print(name, \"parameter: \", p)"],"metadata":{"id":"IL87im19SNED"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's feed some input `x` to the model. We do this by calling `model(x)`(which calls `model.forward(x)` underneath plus other hooks)."],"metadata":{"id":"ezSjAWp6Shlr"}},{"cell_type":"code","source":["# batch of random 16 numbers\n","x = torch.rand(16, 1) \n","y = model(x)\n","print(y)"],"metadata":{"id":"5Syt02gMce4Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Implement optimizer\n","We write a simple Gradient Descent optimizer below. It takes the model parameters and learning rate as arguments. \n","\n","**TODO**: Implement the `step` function, which updates the parameters $w$ and $b$ using the current gradient of the loss (with respect to the parameters) and the learning rate $\\eta$: \n","$$w^{(t+1)}=w^{(t)} - \\eta\\nabla_wE(w^{(t)})$$\n","$$b^{(t+1)}=b^{(t)} - \\eta\\nabla_bE(b^{(t)})$$\n","\n","Note: The `step` function assumes that `backprop` was already called and the gradients of the loss with respect to the parameters `w` and `b` are already computed."],"metadata":{"id":"zR8rwTfUI0mF"}},{"cell_type":"code","metadata":{"id":"UEh2X8-oESU7"},"source":["class GD:\n","  \"\"\"\n","  Gradient Descent optimizer\n","  We will write our own class for now, but in the future we will use\n","  the Optimizers implemented in Pytorch:\n","  https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer \n","  \"\"\"\n","  def __init__(self, params: Iterator[nn.Parameter], lr: int):\n","    self.w, self.b = list(params)\n","    self.lr = lr\n","\n","    # We'll use these two for a plot :)\n","    if len(self.w.shape) == 1:\n","      self.w_hist = [self.w.item()]\n","      self.b_hist = [self.b.item()]\n","\n","  def step(self):\n","    \"\"\"\n","    TODO: Perform a gradient decent step. Update the parameters w and b by using:\n","     - the gradient of the loss with respect to the parameters\n","     - the learning rate\n","    This method is called after backward(), so the gradient of the loss wrt \n","    the parameters is already computed (but where is it stored?)\n","    \"\"\"\n","    with torch.no_grad():\n","      self.w -= ... #TODO\n","      self.b -= ... #TODO\n","\n","    # We'll use these two for a plot :)\n","    if len(self.w.shape) == 1:\n","      self.w_hist.append(self.w.item())\n","      self.b_hist.append(self.b.item())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Implement train routine"],"metadata":{"id":"wPlb0mKDMF1k"}},{"cell_type":"code","metadata":{"id":"U6iDjZ_O0kD3"},"source":["def train(model: GDLinearRegression, data: torch.Tensor, \n","          target: torch.Tensor, optim: GD, criterion: MSE):\n","  \"\"\"Linear Regression train routine\"\"\"\n","  # TODO pass: compute predictions\n","  predictions = ...\n","\n","  # TODO forward pass: compute loss (hint: use criterion)\n","  loss = ...\n","\n","  # TODO backpropagation: compute gradients of loss wrt weights\n","  ...\n","  \n","  # TODO gradient descent step: update weights using the previously computed\n","  # gradients (hint: use optim)\n","  ...\n","\n","  # TODO reset the gradients (hint: use model object)\n","  ...\n","  \n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8CX9WsgkyfwN"},"source":["Let's get some data and plot the *closed-form Linear Regressor*."]},{"cell_type":"code","metadata":{"id":"u-ECH8gZye_M"},"source":["# number of dataset points\n","N = 36 #@param {type:\"slider\", min:10, max:100, step:1}\n","\n","X, y, coef = make_regression(n_samples = N, n_features=1, noise=20, coef=True)\n","closed_form = LinearRegression().fit(X, y)\n","cf_prediction = closed_form.predict(X)\n","plt.scatter(X, y)\n","plt.plot(X, cf_prediction, color='red', label='Closed form')\n","plt.legend(loc='lower right')\n","\n","X = torch.tensor(X).float()\n","y = torch.tensor(y).float()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KxdRabGuwmg5"},"source":["Now let's plot the Linear Regressor obtained by Gradient Descent, at each optimization step.\n","\n","Feel free to play with the learning rate and total optimization steps and see how the results compare.\n","\n","What happens if you set a (very) big learning rate? (>1)"]},{"cell_type":"code","metadata":{"id":"I0HZIFkFwTUS"},"source":["lr = 0.086 #@param {type: \"slider\", min: 0.001, max: 2, step: 0.005}\n","total_steps = 52 #@param {type:\"slider\", min: 0, max: 100, step: 1}\n","\n","model = GDLinearRegression()\n","optimizer = GD(model.parameters(), lr=lr)\n","criterion = MSE()\n","\n","fig, ax = plt.subplots()\n","plt.close()\n","\n","gd_axis, = ax.plot([], [], label='Gradient Descent')\n","cf_axis, = ax.plot([], [], label='Closed Form')\n","legend = ax.legend(loc=2, prop={'size': 10})\n","\n","def init():\n","  scatter = ax.scatter(X, y)\n","  cf_axis.set_data(X, cf_prediction)\n","  return (scatter, cf_axis,)\n","\n","def train_and_plot(step: int):\n","  with torch.no_grad():\n","    y_pred = model(X)\n","  gd_axis.set_data(X, y_pred)\n","  \n","  train(model, X, y, optimizer, criterion)\n","  \n","  legend.texts[0].set_text(f\"Gradient Descent (step={step})\")\n","  legend.texts[1].set_text(\"Closed Form\")\n","\n","  return (gd_axis, legend, )\n","\n","anim = animation.FuncAnimation(\n","    fig, train_and_plot, init_func=init, \n","    frames=total_steps, interval=total_steps*2, blit=True\n","    )\n","\n","rc('animation', html='jshtml')\n","anim"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T8tm_zDdDkW1"},"source":["Let's see how the weight `w` and bias `b` changed after every Gradient Descent step."]},{"cell_type":"code","metadata":{"id":"2_YRRflDCVPG"},"source":["plt.plot(optimizer.w_hist, label='Weight', color='fuchsia')\n","plt.plot(optimizer.b_hist, label='Bias', color='green')\n","plt.xlabel('Gradient Descent steps')\n","plt.ylabel('Value')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xp47fj4W6GkX"},"source":["# Part II: Neural Networks, finally\n","\n","In this section we will implement Neural Networks in PyTorch using predefined `nn.Linear` layers."]},{"cell_type":"markdown","metadata":{"id":"1v0N8Gc2kuF7"},"source":["## Defining some clusters"]},{"cell_type":"markdown","metadata":{"id":"eexHCBoTuYC2"},"source":["We want to train a model that can learn to classify 2D points sampled from 2 clusters in a plane, for example:\n","\n","![Plot showing the distribution](https://i.imgur.com/XwavfqU.png)\n","\n","These clusters are 2D Normal Distributions, $\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, where $\\boldsymbol{\\mu} \\in \\mathbb{R}^2$ defines the center of the distribution, and $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{2 \\times 2}$ is a diagonal matrix containing the standard deviations over each direction. In other words, $\\boldsymbol{\\Sigma} = \\text{diag}(\\boldsymbol{\\sigma^2})$, where $\\sigma_i^2 \\in [0, +\\infty)$ for all $i = \\overline{1, 2}$ (See Chapter 2.3 - Bishop) [[2]](https://en.wikipedia.org/wiki/Multivariate_normal_distribution). \n","\n","Let's first define a helper function that will help us plot our 2D data, along with a class that implements a `sample` and a `plot` method."]},{"cell_type":"code","metadata":{"id":"_0PSlMVw6Gke"},"source":["def plot_set(data: np.array, labels: np.array, alpha=1):\n","    \"\"\"Helper function that plots labeled data\"\"\"\n","    plt.scatter(data[:,0], data[:,1], c=labels, cmap='Accent', alpha=alpha)\n","\n","class LabeledDistribution:\n","    \"\"\"This class represents a distribution over (example, label) pairs\"\"\"\n","\n","    def sample(self, n: int) -> tuple:\n","        \"\"\"Sample from the distribution\"\"\"\n","        raise NotImplementedError\n","        \n","    def plot(self, n: int = 1000):\n","        \"\"\"Plot `n` values sampled from the distribution\"\"\"\n","        data, labels = self.sample(n)\n","        plot_set(data, labels.type(torch.float32))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1vaMdPxHzqpu"},"source":["Now let's define a class that helps us define and sample from clusters in a plane. In its initialiser, `mu`, `sigma` and `labels` are lists of the same length as the number of clusters. `mu[i]` and `sigma[i]` define a cluster of label `labels[i]`. In other words, they define a distribution $\\mathcal{N}(\\boldsymbol{\\mu_i}, diag(\\boldsymbol{\\sigma_i}))$."]},{"cell_type":"code","metadata":{"id":"eB8v3mfvyf_a"},"source":["class Clusters(LabeledDistribution):\n","    \"\"\"This class defines labeled normal-distributed clusters in a plane with diagonal covariance matrix\"\"\"\n","    def __init__(self, mu: list = [[-2, -2], [2, 2]], \n","                       sigma: list = [[1, 1], [1, 1]],\n","                       labels: list = [0, 1]):\n","        self._mu = torch.tensor(mu)\n","        self._sigma = torch.tensor(sigma)\n","        self._labels = torch.tensor(labels).type(torch.long)\n","\n","        self.no_cls = len(set(labels)) # the number of classes in our distribution\n","        self.no_clusters = self._mu.shape[0] # the number of clusters in our distribution\n","        \n","    def sample(self, n: int) -> tuple:\n","        data = torch.normal(torch.zeros(n, 2), torch.ones(n, 2)) # take n samples from a standard normal distribution\n","        cluster_idx = torch.randint(0, self.no_clusters, [n]) # randomly assign each observation to a cluster\n","\n","        shifted_data = data * self._sigma[cluster_idx] + self._mu[cluster_idx] # transform each observation such that they are \n","                                                                               # sampled from the distribution of the cluster\n","        labels = self._labels[cluster_idx] # get the label of each point by looking at the label of its cluster\n","\n","        return shifted_data, labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cV7jrPSyG6ke"},"source":["Now we can sample points from our clusters:"]},{"cell_type":"code","metadata":{"id":"yh2EIObaHCml"},"source":["dist = Clusters()\n","points, labels = dist.sample(10)\n","for point, label in zip(points, labels):\n","    x = round(point[0].item(), 2)\n","    y = round(point[1].item(), 2)\n","    print(f'({x}, {y}) belongs to class {label}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bljhp1V7InRw"},"source":["Let's now plot our clusters to make sure we achieved what we wanted in the first place."]},{"cell_type":"code","metadata":{"id":"fGHRDyoO6Gk3"},"source":["# We now have a distribution to sample from :)\n","dist = Clusters()\n","dist.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s-slwOdB5_A1"},"source":["## Single Layer Neural Network"]},{"cell_type":"markdown","metadata":{"id":"0inFYZWa5_Bc"},"source":["### Model definition\n","Let's build a single layer perceptron as a Pytorch `nn.Module`:\n","\n","$$f(\\boldsymbol{x}) = \\sigma(\\boldsymbol{W} \\boldsymbol{x} + \\boldsymbol{b})$$\n","\n","To build a Neural Network in Pytorch we subclass ``nn.Module`` and implement the following methods:\n","1. `__init__` (the initialiser), where we initialise our weights and biases, as well as define our activation function.\n","2. `forward`, which gets called under the hood when we run `model(...)`\n","\n","The `nn.Module` class give us access to a range of methods, like:\n"," - `.forward()`, called when doing a forward pass\n"," - `.train()`, which puts the model in training mode \n"," - `.eval()`, which puts the model in evaluation mode\n"," - `.zero_grad()`, which zeroes the gradients of all the model parameters \n"," - `.parameters()`, which gives an iterable over all the model's parameters\n"]},{"cell_type":"markdown","metadata":{"id":"Crfn8GmP6vDa"},"source":["### Using `nn.Module` (without predefined `nn` layers)\n","We manually construct our linear layer (weights $\\boldsymbol{W}$ and bias $\\boldsymbol{b}$) for didactic reasons.\n"]},{"cell_type":"code","metadata":{"id":"wz3_4UPQ5_Bh"},"source":["class OneLayer(nn.Module):\n","  \"\"\"\n","  Single-Layer Neural Network, without Pytorch nn.Linear Module\n","  \"\"\"\n","  def __init__(self, \n","               input_size: int, \n","               output_size: int, \n","               activation_fn = lambda x: torch.softmax(x,dim=-1)):\n","      super().__init__()\n","\n","      # randomly initialise the weights with samples from a Gaussian distribution\n","      self._w = nn.Parameter(torch.randn([input_size, output_size]))\n","\n","      # ... and the biases with zeros\n","      self._b = nn.Parameter(torch.zeros([1, output_size]))\n","\n","      self._params = [self._w, self._b]\n","      \n","      # define the activation function\n","      self._activation_fn = activation_fn\n","      \n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","      # Ïƒ(wx + b)\n","      return self._activation_fn(x @ self._w + self._b)\n","\n","  def zero_grad(self):\n","      \"\"\"Reset the gradients of the parameters\"\"\"\n","      for param in self._params:\n","          if param.grad is not None:\n","              param.grad.zero_()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Using nn.Module with predefined layers\n","In practice, defining the Neural Networks layers and implementing them ourselves can get cumbersome. Therefore, we use predefined layers (linear, convolution, recurrent) and activations in the [`torch.nn`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) package.\n","\n","For instance, instead of defining two parameters for the linear layer, we instantiate a [`nn.Linear`](https://pytorch.org/docs/stable/nn.html?highlight=nn%20linear#torch.nn.Linear) object, which takes the number of input features and output features. We then simply feed the input to the `Linear` object in the `forward()` method to get the output."],"metadata":{"id":"9_k7taLmjx1j"}},{"cell_type":"code","metadata":{"id":"lDNjKtM0ggcz"},"source":["class OneLayer(nn.Module):\n","  \"\"\"Single-Layer Neural Network using Pytorch nn.Linear\"\"\"\n","  def __init__(self, \n","               input_size: int, \n","               output_size: int, \n","               activation_fn = lambda x: torch.softmax(x,dim=-1)):\n","      super().__init__()\n","\n","      # we instantiate a Linear layer\n","      # look up nn.Linear in Pytorch documentation:\n","      #   https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n","      self.linear_tr = nn.Linear(\n","          in_features=input_size,\n","          out_features=output_size, \n","          bias=True\n","      )\n","      \n","      # store the activation function\n","      self._activation_fn = activation_fn\n","      \n","  def forward(self, x: torch.Tensor) -> torch.Tensor:     \n","      # step 1: apply linear layer to x\n","      output = self.linear_tr(x)\n","\n","      # step 2: apply softmax to output\n","      y = self._activation_fn(output)\n","\n","      return y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R7Ab9_itYoxf"},"source":["With the hard part out of the way, we now just define two helper functions:\n","1. `plot_decision` simply plots the decision boundary given a model. it does that by predictiong the class every point in the plane\n","2. `plot_loss` simply plots the evolution of our loss function"]},{"cell_type":"code","metadata":{"id":"pxSmHjLoXhtP"},"source":["def plot_decision(model: nn.Module, bounds: list = [-8, 8, -8, 8],\n","                  detail: int = 100):\n","    \"\"\"Plot the decision boundary of the `model`\"\"\"\n","\n","    # Get all the points in the region we want to plot\n","    x1, x2 = torch.meshgrid(torch.linspace(bounds[0], bounds[1], detail),\n","                            torch.linspace(bounds[2], bounds[3], detail))\n","    \n","    data = torch.cat((x1.contiguous().view(-1, 1), x2.contiguous().view(-1, 1)),\n","                     dim=1)\n","    \n","    # use the model to predict the class of every point\n","    labels = torch.argmax(model(data), dim=-1)\n","\n","    # plot all of the points\n","    plot_set(data, labels, alpha=0.1)\n","    \n","def plot_loss(loss: list, label: str, color: str = 'blue'):\n","    \"\"\"Plot the evolution of the loss function\"\"\"\n","    plt.plot(loss, label=label, color=color)\n","    plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NWiZqRILZo-t"},"source":["Let's now plot the computation graph of our network, using a `in_size` of 2 (the `x` and `y` coordinates of every point), and an `out_size` of 2 (the scores the model attributes to the point belonging to each of the 2 classes). To predict the class of the point, we will simply take the `argmax` over the output of the network.\n","\n","We can see from the computation graph all the operations being performed using the weights and biases."]},{"cell_type":"code","metadata":{"id":"UJzKQJIl5_CD"},"source":["model = OneLayer(2,2)\n","\n","x = torch.empty(2)\n","torchviz.make_dot(model(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7JpXC5Icavuq"},"source":["Let's now plot the decision boundary of our randomly initialized model. Unless you are very lucky, the model won't explain the data very well:"]},{"cell_type":"code","metadata":{"id":"WXijzkJ3a5Jm"},"source":["plot_decision(model)\n","dist.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kjMoSgNTbOOK"},"source":["### Model training\n","It's finally time to train our model! First let's generate a sample from our distribution to use as training data."]},{"cell_type":"code","metadata":{"id":"URp7UsRPbq_9"},"source":["train_data, train_labels = dist.sample(100)\n","plot_set(train_data, train_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m3baJY_Cbwcy"},"source":["Now let's define the loss function that we will use to train our model. We will manually define the Negative Log-Likelihood - NLL (though, in practice, this is already implemented in PyTorch).\n","\n","The network outputs a tensor $\\boldsymbol{p} = (p_1, p_2)$, representing the class likelihoods of each of the two classes. Let $p_i \\in [0, 1]$ be the class score of the correct class. Then the NLL is defined as:\n","$$ \\text{NLL} (\\boldsymbol{p}) = - \\log p_i $$\n","\n","When calculating the loss over the whole data, we will simply compute the mean of the losses over every point."]},{"cell_type":"code","metadata":{"id":"iSOUJ9kTjM40"},"source":["def NLL(output: torch.Tensor, true_labels: torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    Given the predictions of the neural network and the ground truth data,\n","    compute the negative log-likelihood.\n","    Arguments:\n","      output: output of neural network, tensor of size N x 2\n","      true_labels: ground truth data, Tensor of size N\n","    \"\"\"\n","    # get the likelihoods of the correct class for all predictions\n","    likelihood = output.gather(1, true_labels.view(-1,1))\n","\n","    # calculate the mean of loss\n","    loss = -torch.log(likelihood)\n","    loss = torch.mean(loss)\n","\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WiXrcrTZ5_Cl"},"source":["NUM_EPOCHS = 400 # the number of epochs to train our model for\n","PRINT_EVERY = 10 # output the results every 100th epoch\n","\n","train_loss = [] # we will store the loss function values at each epoch in this list\n","\n","optim = GD(model.parameters(), lr=0.1) # we will use the `GD` class you defined in Exercise 1 as our optimiser\n","\n","for i in range(NUM_EPOCHS):\n","    output = model(train_data) # run the model over the data to generate the class likelihoods\n","    \n","    loss = NLL(output, train_labels) # compute the loss over the current epoch\n","    \n","    loss.backward() # perform backprop over the loss to compute the gradients of the parameters\n","    optim.step()      # run an optimisation step\n","    model.zero_grad() # reset the gradients, otherwise they will accumulate\n","    \n","    train_loss.append(loss.detach().numpy()) # store the loss of the current epoch\n","\n","    if i % PRINT_EVERY == 0 or i == NUM_EPOCHS - 1: # every once in a while plot the decision boundary and print the loss\n","        print(f'EPOCH {i}:')\n","        print(f'loss = {loss.item()}')\n","        plot_decision(model)\n","        plot_set(train_data, train_labels)\n","        plt.show()\n","        \n","plot_loss(train_loss, 'train-loss')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HZ_BeLXIugqt"},"source":["As you can see, by using only one layer, the model is unable to learn decision boundaries more complex than a simple line. Therefore given the XOR distribution below, no `OneLayer` model can successfuly learn it.\n","\n","Note that the XOR distribution is simply formed out of four clusters being assigned 2 different alternating classes."]},{"cell_type":"code","metadata":{"id":"RUPFONpZ5_Cv"},"source":["xor = Clusters(mu = [[-2, -2], [2, 2], [-2, 2], [2, -2]],\n","              sigma = [[1, 1], [1, 1], [1, 1], [1, 1]],\n","              labels = [0, 0, 1, 1])\n","xor.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rm14kEHd5ckW"},"source":["Because the two classes aren't linearly separable, a Neural Network with a single layer won't manage to learn the XOR distribution. But, for fun, let's actually check that this is true, by instantiating one and training it:"]},{"cell_type":"code","metadata":{"id":"GyTpSW7xvUoM"},"source":["model = OneLayer(2,2)\n","train_data, train_labels = xor.sample(100) # Let's sample our train data from the XOR distribution\n","\n","NUM_EPOCHS = 400\n","PRINT_EVERY = 100\n","\n","train_loss = []\n","\n","optim = GD(model.parameters(), lr=0.1)\n","\n","for i in range(NUM_EPOCHS):\n","    output = model(train_data)\n","    \n","    loss = NLL(output, train_labels)\n","    \n","    loss.backward()\n","    optim.step()\n","    model.zero_grad()\n","    \n","    train_loss.append(loss.detach().numpy())\n","\n","    if i % PRINT_EVERY == 0 or i == NUM_EPOCHS - 1:\n","        print(f'EPOCH {i}:')\n","        print(f'loss = {loss.item()}')\n","        plot_decision(model)\n","        plot_set(train_data, train_labels)\n","        plt.show()\n","        \n","plot_loss(train_loss, 'train-loss')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qSaUf3Aktvkf"},"source":["## Exercise 2: Learning the XOR distribution\n","Given the XOR distribution above, and keeping the `OneLayer` skeleton in mind, define a multi-layer feed-forward network that is capable of learning the XOR distribution and then train it, plotting the decision boundary and the evolution of the loss over the epochs."]},{"cell_type":"code","metadata":{"id":"5MwgqE2A5_DB"},"source":["class MultiLayer(nn.Module):\n","  \"\"\"Multi-Layer Neural Network (1 hidden layer)\"\"\"\n","  def __init__(self, \n","               input_size: int, \n","               hidden_size: int, \n","               output_size: int, \n","               hidden_activation_fn = nn.Tanh(),\n","               output_activation_fn = nn.Softmax(dim=-1)):\n","      \"\"\"\n","      initialize weights and biases for the hidden and the output layers\n","      Arguments:\n","        input_size: number of input neurons\n","        hidden_size: number of neurons of hidden layer \n","        output_size: number of neurons of output layer\n","      \"\"\"\n","      super().__init__()\n","\n","      # Now we'll have two sets of weights and biases, corresponding to two layers\n","      # 1st layer\n","      self._w1 = nn.Parameter(torch.randn([input_size, hidden_size]))\n","      self._b1 = nn.Parameter(torch.zeros([1, hidden_size]))\n","      \n","      # 2nd layer\n","      self._w2 = nn.Parameter(torch.randn([hidden_size, output_size]))\n","      self._b2 = nn.Parameter(torch.zeros([1, output_size]))\n","\n","      self._params = [self._w1, self._w2, self._b1, self._b2]\n","\n","      self._hidden_activation_fn = hidden_activation_fn\n","      self._output_activation_fn = output_activation_fn\n","      \n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","      \"\"\"\n","      Feedforward through the two layers.\n","      Arguments:\n","        x: tensor of size (number_of_examples x self.input_size)\n","      \n","      Returns a tensor of size self.output_size, which is the output of the\n","      network after the softmax activation.\n","      \"\"\"\n","      # Layer 1\n","      x = x @ self._w1 + self._b1\n","      x = self._hidden_activation_fn(x)\n","\n","      # Layer 2\n","      x = x @ self._w2 + self._b2\n","      x = self._output_activation_fn(x)\n","\n","      return x\n","  \n","  def zero_grad(self):\n","      \"\"\"Reset the gradients of the parameters\"\"\"\n","      for param in self._params:\n","          if param.grad is not None:\n","              param.grad.zero_()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yUKL3ATfJG51"},"source":["class MultiLayer(nn.Module):\n","  \"\"\"TODO: Multi-Layer Neural Network (1 hidden layer), using nn.Linear\"\"\"\n","  def __init__(self, \n","               input_size: int, \n","               hidden_size: int, \n","               output_size: int, \n","               hidden_activation_fn = nn.Tanh(),\n","               output_activation_fn = nn.Softmax(dim=-1)):\n","      \"\"\"\n","      TODO: initialize weights and biases for the hidden and the output layers\n","      Arguments:\n","        input_size: number of input neurons\n","        hidden_size: number of neurons of hidden layer \n","        output_size: number of neurons of output layer\n","      \"\"\"\n","      super().__init__()\n","      # TODO: initialize the first linear layer\n","      # self._fst_linear = ...\n","      \n","      # TODO: initialize the second linear layer\n","      # self._snd_linear = ...\n","\n","      self._hidden_activation_fn = hidden_activation_fn\n","      self._output_activation_fn = output_activation_fn\n","      \n","  def forward(self, x: torch.Tensor) -> torch.Tensor:\n","      \"\"\"\n","      Feedforward through the two layers.\n","      Arguments:\n","        x: tensor of size (number_of_examples x self.input_size)\n","      \n","      Returns a tensor of size self.output_size, which is the output of the\n","      network after the softmax activation.\n","      \"\"\"\n","      # TODO: apply first linear layer, then activation\n","      # h = ...\n","\n","      # TODO: apply second linear layer, then activation\n","      # o = ...\n","      \n","      return o"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mwMxUp-YuD2t"},"source":["def NLL(output: torch.Tensor, true_labels: torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    Given the predictions of the neural network and the ground truth data,\n","    compute the negative log-likelihood.\n","    Arguments:\n","      output: output of neural network, tensor of size N x 2\n","      true_labels: ground truth data, Tensor of size N\n","    \"\"\"\n","    likelihood = output.gather(1, true_labels.view(-1,1))\n","\n","    loss = -torch.log(likelihood)\n","    loss = torch.mean(loss)\n","\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xLQm59s3LNSv"},"source":["# We need to perform Gradient Descent on 2 layers now, so let's redefine our `GD` class\n","class GD:\n","  def __init__(self, params: torch.Tensor, lr: int):\n","    self.w1, self.w2, self.b1, self.b2 = list(params)\n","    self.lr = lr\n","\n","  def step(self):\n","    with torch.no_grad():\n","      self.w1 -= self.lr * self.w1.grad\n","      self.b1 -= self.lr * self.b1.grad\n","      self.w2 -= self.lr * self.w2.grad\n","      self.b2 -= self.lr * self.b2.grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_wTm5Q0l4q0U"},"source":["# For fun, use `torchviz.make_dot()` to see the computation graph of the network\n","INPUT_SIZE = 2\n","HIDDEN_SIZE = 5\n","OUTPUT_SIZE = 2\n","x = torch.randn(INPUT_SIZE)\n","model = MultiLayer(input_size=INPUT_SIZE, \n","                   hidden_size=HIDDEN_SIZE,\n","                   output_size=OUTPUT_SIZE)\n","torchviz.make_dot(model(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eMTuuR3nuDsM"},"source":["# Finally, train the network and plot the decision boundaries\n","model = MultiLayer(input_size=INPUT_SIZE, \n","                   hidden_size=HIDDEN_SIZE,\n","                   output_size=OUTPUT_SIZE)\n","train_data, train_labels = xor.sample(100) # Let's sample our train data from the XOR distribution\n","\n","NUM_EPOCHS = 400\n","PRINT_EVERY = 100\n","\n","train_loss = []\n","\n","optim = GD(model.parameters(), lr=0.1)\n","\n","for i in range(NUM_EPOCHS):\n","    output = model(train_data)\n","    loss = NLL(output, train_labels)\n","    \n","    loss.backward()\n","    optim.step()\n","    model.zero_grad()\n","    \n","    train_loss.append(loss.detach().numpy())\n","\n","    if i % PRINT_EVERY == 0 or i == NUM_EPOCHS - 1:\n","        print(f'EPOCH {i}:')\n","        print(f'loss = {loss.item()}')\n","        plot_decision(model)\n","        plot_set(train_data, train_labels)\n","        plt.show()\n","        \n","plot_loss(train_loss, 'train-loss')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sdGwOkTWdq5l"},"source":["## Exercise 3: Learning three clusters\n","\n","Define a Neural Network that can successfully classify samples belonging to these three clusters:\n","\n","![3 clusters in a plane](https://i.imgur.com/EnFKfZG.png)"]},{"cell_type":"markdown","metadata":{"id":"3ltL-yKWeHbz"},"source":["First up, let's define our clusters:\n"]},{"cell_type":"code","metadata":{"id":"z-qqu3hQeLTp"},"source":["threeClass = Clusters(mu = [[-2,-2],[2,2],[-2,2]],\n","                      sigma = [[1,1],[1,1],[1,1]],\n","                      labels = [0,1,2])\n","threeClass.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xE3oGD63eMsj"},"source":["Finally, let's sample a train dataset and a validation dataset:"]},{"cell_type":"code","metadata":{"id":"JTDZTMP8eRyW"},"source":["train_data, train_labels = threeClass.sample(200)\n","val_data, val_labels = threeClass.sample(50)\n","\n","plot_set(train_data, train_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DQI2-DTOeeI_"},"source":["You should:\n","1. Define the Neural Network, inheriting `torch.nn.Module` and using all the appropriate objects provided by torch. You can use one of the loss functions already provided by `torch.nn.functional` [[5]](https://pytorch.org/docs/stable/nn.functional.html);\n","2. For fun, plot the computation graph;\n","3. Train the network like before, printing the loss and decision boundary from time to time. For now, you can use the `torch.optim.SGD` optimiser [[6]](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD);\n","4. Take a sample from the distribution and use it as a validation dataset, testing the loss function on it without training every epoch. Plot both losses;"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"wGwyO1ZMQwh9"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PpPw4R5PW9X6"},"source":["class ThreeClassNN(nn.Module):\n","    def __init__(self, \n","                 input_size: int, \n","                 hidden_size: int, \n","                 output_size: int,\n","                 hidden_activation_fn = nn.ReLU()):\n","        # Initialise the base class (nn.Module)\n","        super().__init__()\n","\n","        # TODO use `torch.nn.Linear` to instantiate the hidden and the output\n","        # layer. Look up the Pytorch documentation:\n","        #   https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n","        # self._layer1 = ...\n","        # self._layer2 = ...\n","\n","        self._hidden_activation = hidden_activation_fn\n","   \n","    def forward(self, x):\n","        # TODO: apply first layer transformation + activation\n","        # h = ...\n","\n","        # TODO: apply second layer transformation\n","        # Because we will use CrossEntropy as our loss, we don't need a\n","        # a softmax activation function after layer 2\n","        out = ...\n","        \n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sn1CnRpyfJ_7"},"source":["# For fun, use `torchviz.make_dot()` to see the computation graph of the network\n","model = ThreeClassNN(2, 4, 3)\n","x = torch.randn(2)\n","#torchviz.make_dot(model(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F4mWV646fKAd"},"source":["# Finally, train the network and plot the decision boundaries\n","model = ThreeClassNN(2, 4, 3)\n","\n","NUM_EPOCHS = 400\n","PRINT_EVERY = 20\n","\n","train_loss = []\n","val_loss = [] # This time we will track the loss on val_data\n","\n","# TODO: instantiante SGD optimizer with a learning rate of 0.01\n","# look up the Pytorch documentation:\n","# https://pytorch.org/docs/stable/optim.html?highlight=sgd#torch.optim.SGD\n","# optim = ...\n","\n","# TODO: instantiate the Cross Entropy loss\n","# look up the Pytorch documentation:\n","# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n","# criterion = ...\n","\n","for i in range(NUM_EPOCHS):\n","    # Set the model to train mode and reset the gradients\n","    model.train()\n","    \n","\n","    output = model(train_data)\n","    \n","    # The main difference between Cross Entropy and NLL is that the first doesn't\n","    # expect the output to be class likelihoods (\\in [0, 1]), but rather\n","    # class scores (\\in \\mathbb{R}). That's why we didn't use softmax this time\n","    # on the last layer.\n","    loss = criterion(output, train_labels)\n","    \n","    loss.backward()\n","    optim.step()\n","    model.zero_grad()\n","    \n","    train_loss.append(loss.detach().numpy())\n","\n","    if i % PRINT_EVERY == 0 or i == NUM_EPOCHS - 1:\n","        print(f'EPOCH {i}:')\n","        print(f'loss = {loss.item()}')\n","        plot_decision(model)\n","        #plot_set(train_data, train_labels)\n","        plot_set(val_data, val_labels)\n","        plt.show()\n","\n","    # Every epoch, let's evaluate our model on the validation data and plot both losses at the end\n","    model.eval() # set the model to evaluation mode\n","    with torch.no_grad():\n","        output = model(val_data)\n","        validation_loss = F.cross_entropy(output, val_labels)\n","        val_loss.append(validation_loss)\n","\n","plot_loss(train_loss, 'train-loss')\n","plot_loss(val_loss, 'val-loss', color='green')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IN_roqt4Mo0s"},"source":["|"],"execution_count":null,"outputs":[]}]}